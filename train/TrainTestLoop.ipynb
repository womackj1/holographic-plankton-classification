{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "mounted-sydney",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\lnyman2012\\appdata\\local\\continuum\\anaconda3\\envs\\automated_class\\lib\\site-packages\\torchvision\\transforms\\functional.py:92: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ..\\torch\\csrc\\utils\\tensor_numpy.cpp:141.)\n",
      "  img = torch.from_numpy(np.array(pic, np.float32, copy=False))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Phase: train loss: 0.9430444129965891, acc: 0.9214876033057852\n",
      "Epoch: 0 Phase: test loss: 0.046454186570515074, acc: 0.9924242424242424\n",
      "Epoch: 1 Phase: train loss: 0.007468565392334936, acc: 0.9984504132231405\n",
      "Epoch: 1 Phase: test loss: 0.0020085748735803742, acc: 0.9986225895316805\n",
      "Epoch: 2 Phase: train loss: 0.0070263222395406345, acc: 0.9989669421487604\n",
      "Epoch: 2 Phase: test loss: 0.0001633162206146589, acc: 1.0\n",
      "Epoch: 3 Phase: train loss: 6.724680567095893e-05, acc: 1.0\n",
      "Epoch: 3 Phase: test loss: 2.982334326062744e-05, acc: 1.0\n",
      "Epoch: 4 Phase: train loss: 9.916939677559444e-06, acc: 1.0\n",
      "Epoch: 4 Phase: test loss: 1.579343491410192e-06, acc: 1.0\n",
      "Epoch: 5 Phase: train loss: 1.3373838161099932e-05, acc: 1.0\n",
      "Epoch: 5 Phase: test loss: 0.00024332424244533264, acc: 1.0\n",
      "Epoch: 6 Phase: train loss: 1.0635699243345848e-05, acc: 1.0\n",
      "Epoch: 6 Phase: test loss: 9.838295614951098e-07, acc: 1.0\n",
      "Epoch: 7 Phase: train loss: 1.471031906893469e-06, acc: 1.0\n",
      "Epoch: 7 Phase: test loss: 6.984105863849879e-07, acc: 1.0\n",
      "Epoch: 8 Phase: train loss: 5.33394094009941e-07, acc: 1.0\n",
      "Epoch: 8 Phase: test loss: 2.2716290803236165e-07, acc: 1.0\n",
      "Epoch: 9 Phase: train loss: 0.048748559913707225, acc: 0.9848484848484849\n",
      "Epoch: 9 Phase: test loss: 0.000260480112888781, acc: 1.0\n",
      "Epoch: 10 Phase: train loss: 0.2447526934169875, acc: 0.9815771349862259\n",
      "Epoch: 10 Phase: test loss: 79.83214934798312, acc: 0.9042699724517906\n",
      "Epoch: 11 Phase: train loss: 1.3528760095140058, acc: 0.96883608815427\n",
      "Epoch: 11 Phase: test loss: 0.0002179406270473062, acc: 1.0\n",
      "Epoch: 12 Phase: train loss: 0.05301014425254442, acc: 0.9838154269972452\n",
      "Epoch: 12 Phase: test loss: 0.011831211603500626, acc: 0.9986225895316805\n",
      "Epoch: 13 Phase: train loss: 0.027377393945274973, acc: 0.9925964187327824\n",
      "Epoch: 13 Phase: test loss: 0.005482150800500586, acc: 0.9972451790633609\n",
      "Epoch: 14 Phase: train loss: 0.004928604129451812, acc: 0.9989669421487604\n",
      "Epoch: 14 Phase: test loss: 7.434668705999766e-05, acc: 1.0\n",
      "Epoch: 15 Phase: train loss: 0.008014993356664632, acc: 0.9975895316804407\n",
      "Epoch: 15 Phase: test loss: 0.00228118374539647, acc: 0.9993112947658402\n",
      "Epoch: 16 Phase: train loss: 0.0002258751969043448, acc: 0.9998278236914601\n",
      "Epoch: 16 Phase: test loss: 0.00276563406107978, acc: 0.9993112947658402\n",
      "Epoch: 17 Phase: train loss: 0.0002767007482659169, acc: 0.9998278236914601\n",
      "Epoch: 17 Phase: test loss: 0.0063077329243678235, acc: 0.9993112947658402\n",
      "Epoch: 18 Phase: train loss: 3.1504154319862135e-05, acc: 1.0\n",
      "Epoch: 18 Phase: test loss: 0.004884247362617527, acc: 0.9993112947658402\n",
      "Epoch: 19 Phase: train loss: 6.476411217893509e-07, acc: 1.0\n",
      "Epoch: 19 Phase: test loss: 9.962784241926035e-05, acc: 1.0\n",
      "Epoch: 20 Phase: train loss: 2.600129355386351e-07, acc: 1.0\n",
      "Epoch: 20 Phase: test loss: 3.880707124252448e-05, acc: 1.0\n",
      "Epoch: 21 Phase: train loss: 4.388884352695555e-07, acc: 1.0\n",
      "Epoch: 21 Phase: test loss: 0.0048784934619980755, acc: 0.9993112947658402\n",
      "Epoch: 22 Phase: train loss: 3.5932569215763825e-07, acc: 1.0\n",
      "Epoch: 22 Phase: test loss: 0.0046436468102867366, acc: 0.9993112947658402\n",
      "Epoch: 23 Phase: train loss: 4.820291368701396e-07, acc: 1.0\n",
      "Epoch: 23 Phase: test loss: 0.004682852791685092, acc: 0.9993112947658402\n",
      "Epoch: 24 Phase: train loss: 0.00012089484664918092, acc: 0.9998278236914601\n",
      "Epoch: 24 Phase: test loss: 0.00910602046136142, acc: 0.9986225895316805\n",
      "Epoch: 25 Phase: train loss: 0.005777771461414915, acc: 0.9982782369146006\n",
      "Epoch: 25 Phase: test loss: 0.0014455742212320286, acc: 0.9993112947658402\n",
      "Epoch: 26 Phase: train loss: 0.0032127143889080553, acc: 0.9989669421487604\n",
      "Epoch: 26 Phase: test loss: 2.3312794732462426e-06, acc: 1.0\n",
      "Epoch: 27 Phase: train loss: 2.846307167878886e-05, acc: 1.0\n",
      "Epoch: 27 Phase: test loss: 8.469942619462213e-06, acc: 1.0\n",
      "Epoch: 28 Phase: train loss: 1.5862678461772625e-05, acc: 1.0\n",
      "Epoch: 28 Phase: test loss: 3.0041499725905676e-06, acc: 1.0\n",
      "Epoch: 29 Phase: train loss: 0.002774091301985655, acc: 0.9994834710743802\n",
      "Epoch: 29 Phase: test loss: 7.319709819039756e-05, acc: 1.0\n",
      "Epoch: 30 Phase: train loss: 0.003420217726727786, acc: 0.9996556473829201\n",
      "Epoch: 30 Phase: test loss: 1.7451975343812425e-05, acc: 1.0\n",
      "Epoch: 31 Phase: train loss: 1.800747953821387e-06, acc: 1.0\n",
      "Epoch: 31 Phase: test loss: 2.0423771489310813e-06, acc: 1.0\n",
      "Epoch: 32 Phase: train loss: 3.929641741626634e-06, acc: 1.0\n",
      "Epoch: 32 Phase: test loss: 1.7807307497507793e-06, acc: 1.0\n",
      "Epoch: 33 Phase: train loss: 7.925534537343217e-07, acc: 1.0\n",
      "Epoch: 33 Phase: test loss: 9.032597519823062e-07, acc: 1.0\n",
      "Epoch: 34 Phase: train loss: 4.620616516328063e-07, acc: 1.0\n",
      "Epoch: 34 Phase: test loss: 4.859487626914821e-07, acc: 1.0\n",
      "Epoch: 35 Phase: train loss: 4.065394471238798e-07, acc: 1.0\n",
      "Epoch: 35 Phase: test loss: 3.1518151203730786e-07, acc: 1.0\n",
      "Epoch: 36 Phase: train loss: 5.353799971066385e-07, acc: 1.0\n",
      "Epoch: 36 Phase: test loss: 2.0336100371709153e-07, acc: 1.0\n",
      "Epoch: 37 Phase: train loss: 2.406095253175663e-07, acc: 1.0\n",
      "Epoch: 37 Phase: test loss: 1.2585929448317427e-07, acc: 1.0\n",
      "Epoch: 38 Phase: train loss: 1.4948800016394664e-07, acc: 1.0\n",
      "Epoch: 38 Phase: test loss: 1.0090091139840643e-07, acc: 1.0\n",
      "Epoch: 39 Phase: train loss: 2.201796348440014e-07, acc: 1.0\n",
      "Epoch: 39 Phase: test loss: 6.970292065113224e-08, acc: 1.0\n",
      "Epoch: 40 Phase: train loss: 9.982906694132287e-08, acc: 1.0\n",
      "Epoch: 40 Phase: test loss: 5.730582128202784e-08, acc: 1.0\n",
      "Epoch: 41 Phase: train loss: 5.974701856858632e-08, acc: 1.0\n",
      "Epoch: 41 Phase: test loss: 6.026138427563833e-08, acc: 1.0\n",
      "Epoch: 42 Phase: train loss: 0.0033047543015675904, acc: 0.9998278236914601\n",
      "Epoch: 42 Phase: test loss: 5.541532674792337e-08, acc: 1.0\n",
      "Epoch: 43 Phase: train loss: 0.005063379518186711, acc: 0.9996556473829201\n",
      "Epoch: 43 Phase: test loss: 0.00017095107491612427, acc: 1.0\n",
      "Epoch: 44 Phase: train loss: 0.027834672841255962, acc: 0.9939738292011019\n",
      "Epoch: 44 Phase: test loss: 0.015100769793026896, acc: 0.9958677685950413\n",
      "Epoch: 45 Phase: train loss: 0.03348824294917206, acc: 0.9939738292011019\n",
      "Epoch: 45 Phase: test loss: 0.04285185799880092, acc: 0.9855371900826446\n",
      "Epoch: 46 Phase: train loss: 0.016304687963178615, acc: 0.9951790633608816\n",
      "Epoch: 46 Phase: test loss: 0.001525038400494632, acc: 0.9993112947658402\n",
      "Epoch: 47 Phase: train loss: 0.011690385043006684, acc: 0.9986225895316805\n",
      "Epoch: 47 Phase: test loss: 3.595590806353282e-06, acc: 1.0\n",
      "Epoch: 48 Phase: train loss: 0.0017508413231174605, acc: 0.9996556473829201\n",
      "Epoch: 48 Phase: test loss: 1.4521565935113861e-05, acc: 1.0\n",
      "Epoch: 49 Phase: train loss: 0.0003898167968274159, acc: 0.9996556473829201\n",
      "Epoch: 49 Phase: test loss: 9.212861890367748e-05, acc: 1.0\n",
      "Epoch: 50 Phase: train loss: 0.0005905111405975508, acc: 0.9998278236914601\n",
      "Epoch: 50 Phase: test loss: 0.00039171264958617224, acc: 1.0\n",
      "Epoch: 51 Phase: train loss: 0.0012731573289688654, acc: 0.9996556473829201\n",
      "Epoch: 51 Phase: test loss: 6.5262619921495495e-06, acc: 1.0\n",
      "Epoch: 52 Phase: train loss: 4.015960346947975e-06, acc: 1.0\n",
      "Epoch: 52 Phase: test loss: 5.0932205813897925e-06, acc: 1.0\n",
      "Epoch: 53 Phase: train loss: 1.7388101975201653e-06, acc: 1.0\n",
      "Epoch: 53 Phase: test loss: 2.229517284576112e-06, acc: 1.0\n",
      "Epoch: 54 Phase: train loss: 6.291114937575852e-06, acc: 1.0\n",
      "Epoch: 54 Phase: test loss: 2.13828803672855e-06, acc: 1.0\n",
      "Epoch: 55 Phase: train loss: 9.298589043873007e-07, acc: 1.0\n",
      "Epoch: 55 Phase: test loss: 9.451996804784342e-07, acc: 1.0\n",
      "Epoch: 56 Phase: train loss: 8.653457641248928e-07, acc: 1.0\n",
      "Epoch: 56 Phase: test loss: 1.22919266138246e-06, acc: 1.0\n",
      "Epoch: 57 Phase: train loss: 3.473304100016453e-07, acc: 1.0\n",
      "Epoch: 57 Phase: test loss: 4.526129674176002e-07, acc: 1.0\n",
      "Epoch: 58 Phase: train loss: 3.07005909227319e-07, acc: 1.0\n",
      "Epoch: 58 Phase: test loss: 5.819069648837774e-07, acc: 1.0\n",
      "Epoch: 59 Phase: train loss: 3.5655031016178976e-07, acc: 1.0\n",
      "Epoch: 59 Phase: test loss: 8.266809755162488e-07, acc: 1.0\n",
      "Epoch: 60 Phase: train loss: 2.6170026398639763e-07, acc: 1.0\n",
      "Epoch: 60 Phase: test loss: 3.27818533266569e-07, acc: 1.0\n",
      "Epoch: 61 Phase: train loss: 5.386277248059007e-07, acc: 1.0\n",
      "Epoch: 61 Phase: test loss: 2.2741569776274048e-07, acc: 1.0\n",
      "Epoch: 62 Phase: train loss: 0.0013687407184465213, acc: 0.9994834710743802\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 62 Phase: test loss: 0.00028130241510605493, acc: 1.0\n",
      "Epoch: 63 Phase: train loss: 0.0049442590631556295, acc: 0.9991391184573003\n",
      "Epoch: 63 Phase: test loss: 2.7709908369952447e-05, acc: 1.0\n",
      "Epoch: 64 Phase: train loss: 0.00010154026192178413, acc: 1.0\n",
      "Epoch: 64 Phase: test loss: 6.315139824115176e-07, acc: 1.0\n",
      "Epoch: 65 Phase: train loss: 2.1499460877526348e-07, acc: 1.0\n",
      "Epoch: 65 Phase: test loss: 8.342230569586406e-07, acc: 1.0\n",
      "Epoch: 66 Phase: train loss: 1.4203454915053797e-07, acc: 1.0\n",
      "Epoch: 66 Phase: test loss: 1.1138121543238352e-06, acc: 1.0\n",
      "Epoch: 67 Phase: train loss: 8.096186209640589e-07, acc: 1.0\n",
      "Epoch: 67 Phase: test loss: 1.678840851828843e-07, acc: 1.0\n",
      "Epoch: 68 Phase: train loss: 2.8324418637901803e-08, acc: 1.0\n",
      "Epoch: 68 Phase: test loss: 5.610557900070739e-07, acc: 1.0\n",
      "Epoch: 69 Phase: train loss: 6.013547138670763e-08, acc: 1.0\n",
      "Epoch: 69 Phase: test loss: 1.4358342928212814e-07, acc: 1.0\n",
      "Epoch: 70 Phase: train loss: 4.97186593350175e-07, acc: 1.0\n",
      "Epoch: 70 Phase: test loss: 2.7421344254950917e-08, acc: 1.0\n",
      "Epoch: 71 Phase: train loss: 2.7523895889719988e-08, acc: 1.0\n",
      "Epoch: 71 Phase: test loss: 2.1663718912798944e-07, acc: 1.0\n",
      "Epoch: 72 Phase: train loss: 1.9453033831340247e-07, acc: 1.0\n",
      "Epoch: 72 Phase: test loss: 1.4145080895753208e-07, acc: 1.0\n",
      "Epoch: 73 Phase: train loss: 2.9247958380287358e-08, acc: 1.0\n",
      "Epoch: 73 Phase: test loss: 9.40005838457137e-08, acc: 1.0\n",
      "Epoch: 74 Phase: train loss: 9.483828013087142e-08, acc: 1.0\n",
      "Epoch: 74 Phase: test loss: 1.3381409220728566e-07, acc: 1.0\n",
      "Epoch: 75 Phase: train loss: 2.7482720525408094e-08, acc: 1.0\n",
      "Epoch: 75 Phase: test loss: 1.8602844784536425e-07, acc: 1.0\n",
      "Epoch: 76 Phase: train loss: 7.620076950331975e-08, acc: 1.0\n",
      "Epoch: 76 Phase: test loss: 6.411821183085274e-08, acc: 1.0\n",
      "Epoch: 77 Phase: train loss: 1.169923488114392e-08, acc: 1.0\n",
      "Epoch: 77 Phase: test loss: 2.7517166992757865e-07, acc: 1.0\n",
      "Epoch: 78 Phase: train loss: 1.5024277810840753e-08, acc: 1.0\n",
      "Epoch: 78 Phase: test loss: 1.0943345537005518e-07, acc: 1.0\n",
      "Epoch: 79 Phase: train loss: 6.661996828941338e-08, acc: 1.0\n",
      "Epoch: 79 Phase: test loss: 2.5965702824968716e-07, acc: 1.0\n",
      "Epoch: 80 Phase: train loss: 2.0565975027658642e-08, acc: 1.0\n",
      "Epoch: 80 Phase: test loss: 2.906097246556486e-07, acc: 1.0\n",
      "Epoch: 81 Phase: train loss: 1.1288734178843088e-08, acc: 1.0\n",
      "Epoch: 81 Phase: test loss: 5.1475569870897117e-08, acc: 1.0\n",
      "Epoch: 82 Phase: train loss: 3.2223458722272476e-08, acc: 1.0\n",
      "Epoch: 82 Phase: test loss: 1.7247549745071628e-07, acc: 1.0\n",
      "Epoch: 83 Phase: train loss: 2.167419376757498e-08, acc: 1.0\n",
      "Epoch: 83 Phase: test loss: 1.1658205772658574e-08, acc: 1.0\n",
      "Epoch: 84 Phase: train loss: 1.795917206316894e-08, acc: 1.0\n",
      "Epoch: 84 Phase: test loss: 5.205068959133949e-08, acc: 1.0\n",
      "Epoch: 85 Phase: train loss: 2.1448351304386994e-08, acc: 1.0\n",
      "Epoch: 85 Phase: test loss: 4.4497588350467155e-08, acc: 1.0\n",
      "Epoch: 86 Phase: train loss: 1.3813178736736874e-08, acc: 1.0\n",
      "Epoch: 86 Phase: test loss: 1.4449595532950719e-08, acc: 1.0\n",
      "Epoch: 87 Phase: train loss: 1.8759709414912452e-08, acc: 1.0\n",
      "Epoch: 87 Phase: test loss: 3.5630965667383515e-08, acc: 1.0\n",
      "Epoch: 88 Phase: train loss: 4.542002388888844e-08, acc: 1.0\n",
      "Epoch: 88 Phase: test loss: 1.182240286200885e-08, acc: 1.0\n",
      "Epoch: 89 Phase: train loss: 1.8615974594723168e-08, acc: 1.0\n",
      "Epoch: 89 Phase: test loss: 1.4531697160559215e-08, acc: 1.0\n",
      "Epoch: 90 Phase: train loss: 6.197824138574444e-08, acc: 1.0\n",
      "Epoch: 90 Phase: test loss: 1.0262504102718797e-08, acc: 1.0\n",
      "Epoch: 91 Phase: train loss: 9.934088262741249e-09, acc: 1.0\n",
      "Epoch: 91 Phase: test loss: 9.44150575168922e-09, acc: 1.0\n",
      "Epoch: 92 Phase: train loss: 8.805220758216733e-09, acc: 1.0\n",
      "Epoch: 92 Phase: test loss: 1.3299145833074857e-07, acc: 1.0\n",
      "Epoch: 93 Phase: train loss: 1.5681011042457295e-08, acc: 1.0\n",
      "Epoch: 93 Phase: test loss: 1.0755104149789377e-08, acc: 1.0\n",
      "Epoch: 94 Phase: train loss: 5.442914990709134e-08, acc: 1.0\n",
      "Epoch: 94 Phase: test loss: 1.034460473204411e-08, acc: 1.0\n",
      "Epoch: 95 Phase: train loss: 1.7096944693776566e-08, acc: 1.0\n",
      "Epoch: 95 Phase: test loss: 3.62055369486622e-08, acc: 1.0\n",
      "Epoch: 96 Phase: train loss: 1.6522552538266278e-08, acc: 1.0\n",
      "Epoch: 96 Phase: test loss: 1.300354828993774e-07, acc: 1.0\n",
      "Epoch: 97 Phase: train loss: 5.993300181208453e-09, acc: 1.0\n",
      "Epoch: 97 Phase: test loss: 8.04580518279707e-09, acc: 1.0\n",
      "Epoch: 98 Phase: train loss: 1.1388872653031709e-07, acc: 1.0\n",
      "Epoch: 98 Phase: test loss: 7.389004434939996e-09, acc: 1.0\n",
      "Epoch: 99 Phase: train loss: 4.4371801722338085e-08, acc: 1.0\n",
      "Epoch: 99 Phase: test loss: 1.5925730137032756e-07, acc: 1.0\n",
      "Epoch: 100 Phase: train loss: 2.050391608565132e-08, acc: 1.0\n",
      "Epoch: 100 Phase: test loss: 3.448174632510491e-08, acc: 1.0\n",
      "Epoch: 101 Phase: train loss: 1.4367435927844306e-08, acc: 1.0\n",
      "Epoch: 101 Phase: test loss: 6.485903619505727e-09, acc: 1.0\n",
      "Epoch: 102 Phase: train loss: 5.541748125590036e-09, acc: 1.0\n",
      "Epoch: 102 Phase: test loss: 2.348040140282325e-08, acc: 1.0\n",
      "Epoch: 103 Phase: train loss: 4.187100715733582e-09, acc: 1.0\n",
      "Epoch: 103 Phase: test loss: 5.500702923458531e-09, acc: 1.0\n",
      "Epoch: 104 Phase: train loss: 6.178011472466162e-09, acc: 1.0\n",
      "Epoch: 104 Phase: test loss: 4.433402889981905e-09, acc: 1.0\n",
      "Epoch: 105 Phase: train loss: 7.306873537815397e-09, acc: 1.0\n",
      "Epoch: 105 Phase: test loss: 1.2256536181935604e-07, acc: 1.0\n",
      "Epoch: 106 Phase: train loss: 1.8184872902270728e-08, acc: 1.0\n",
      "Epoch: 106 Phase: test loss: 4.105002633498448e-09, acc: 1.0\n",
      "Epoch: 107 Phase: train loss: 1.4326331206748204e-08, acc: 1.0\n",
      "Epoch: 107 Phase: test loss: 1.1238623751373517e-07, acc: 1.0\n",
      "Epoch: 108 Phase: train loss: 1.221221847247011e-08, acc: 1.0\n",
      "Epoch: 108 Phase: test loss: 2.13460122672342e-09, acc: 1.0\n",
      "Epoch: 109 Phase: train loss: 9.934033893009391e-09, acc: 1.0\n",
      "Epoch: 109 Phase: test loss: 1.3874806361686827e-08, acc: 1.0\n",
      "Epoch: 110 Phase: train loss: 1.4018473489467958e-08, acc: 1.0\n",
      "Epoch: 110 Phase: test loss: 4.597599171897254e-09, acc: 1.0\n",
      "Epoch: 111 Phase: train loss: 1.0324017985778645e-08, acc: 1.0\n",
      "Epoch: 111 Phase: test loss: 6.157496037385387e-09, acc: 1.0\n",
      "Epoch: 112 Phase: train loss: 1.838986832052358e-08, acc: 1.0\n",
      "Epoch: 112 Phase: test loss: 2.13460113863961e-09, acc: 1.0\n",
      "Epoch: 113 Phase: train loss: 2.118136554306419e-08, acc: 1.0\n",
      "Epoch: 113 Phase: test loss: 9.194626201277489e-08, acc: 1.0\n",
      "Epoch: 114 Phase: train loss: 1.923141349016148e-08, acc: 1.0\n",
      "Epoch: 114 Phase: test loss: 3.448200388216599e-09, acc: 1.0\n",
      "Epoch: 115 Phase: train loss: 4.74126758790411e-09, acc: 1.0\n",
      "Epoch: 115 Phase: test loss: 8.890879436163619e-08, acc: 1.0\n",
      "Epoch: 116 Phase: train loss: 5.8496209896154805e-09, acc: 1.0\n",
      "Epoch: 116 Phase: test loss: 7.389004082604755e-10, acc: 1.0\n",
      "Epoch: 117 Phase: train loss: 5.0238984399594866e-08, acc: 1.0\n",
      "Epoch: 117 Phase: test loss: 8.324447407242433e-08, acc: 1.0\n",
      "Epoch: 118 Phase: train loss: 5.439112579981254e-09, acc: 1.0\n",
      "Epoch: 118 Phase: test loss: 9.852005639214808e-10, acc: 1.0\n",
      "Epoch: 119 Phase: train loss: 2.8529731855201605e-09, acc: 1.0\n",
      "Epoch: 119 Phase: test loss: 2.2988005915721266e-09, acc: 1.0\n",
      "Epoch: 120 Phase: train loss: 4.322095021274725e-08, acc: 1.0\n",
      "Epoch: 120 Phase: test loss: 1.4778006403533308e-09, acc: 1.0\n",
      "Epoch: 121 Phase: train loss: 6.403785678041547e-09, acc: 1.0\n",
      "Epoch: 121 Phase: test loss: 8.210004650410223e-10, acc: 1.0\n",
      "Epoch: 122 Phase: train loss: 3.0787476810380872e-09, acc: 1.0\n",
      "Epoch: 122 Phase: test loss: 7.864785732862372e-08, acc: 1.0\n",
      "Epoch: 123 Phase: train loss: 2.5040503885285703e-09, acc: 1.0\n",
      "Epoch: 123 Phase: test loss: 4.679690757951384e-09, acc: 1.0\n",
      "Epoch: 124 Phase: train loss: 3.0377005594135513e-09, acc: 1.0\n",
      "Epoch: 124 Phase: test loss: 4.843890842151208e-09, acc: 1.0\n",
      "Epoch: 125 Phase: train loss: 2.955595481855822e-09, acc: 1.0\n",
      "Epoch: 125 Phase: test loss: 4.105002398608287e-10, acc: 1.0\n",
      "Epoch: 126 Phase: train loss: 1.364895295041526e-08, acc: 1.0\n",
      "Epoch: 126 Phase: test loss: 5.5006927791397205e-09, acc: 1.0\n",
      "Epoch: 127 Phase: train loss: 2.257751058653286e-09, acc: 1.0\n",
      "Epoch: 127 Phase: test loss: 2.4630015566100523e-10, acc: 1.0\n",
      "Epoch: 128 Phase: train loss: 1.7404694618494056e-08, acc: 1.0\n",
      "Epoch: 128 Phase: test loss: 4.022895309803557e-09, acc: 1.0\n",
      "Epoch: 129 Phase: train loss: 2.9966476719696045e-09, acc: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 129 Phase: test loss: 9.852005198795756e-10, acc: 1.0\n",
      "Epoch: 130 Phase: train loss: 1.190449725940459e-09, acc: 1.0\n",
      "Epoch: 130 Phase: test loss: 8.210004063184822e-10, acc: 1.0\n",
      "Epoch: 131 Phase: train loss: 2.2988001291321233e-09, acc: 1.0\n",
      "Epoch: 131 Phase: test loss: 9.03100477779664e-10, acc: 1.0\n",
      "Epoch: 132 Phase: train loss: 2.5040483148888715e-09, acc: 1.0\n",
      "Epoch: 132 Phase: test loss: 4.1050025454146373e-10, acc: 1.0\n",
      "Epoch: 133 Phase: train loss: 3.858695522886593e-09, acc: 1.0\n",
      "Epoch: 133 Phase: test loss: 3.4481953380781464e-09, acc: 1.0\n",
      "Epoch: 134 Phase: train loss: 2.052499940439689e-09, acc: 1.0\n",
      "Epoch: 134 Phase: test loss: 8.210004209991172e-10, acc: 1.0\n",
      "Epoch: 135 Phase: train loss: 6.96718257411979e-08, acc: 1.0\n",
      "Epoch: 135 Phase: test loss: 2.5450983396089862e-09, acc: 1.0\n",
      "Epoch: 136 Phase: train loss: 3.4892435864414218e-09, acc: 1.0\n",
      "Epoch: 136 Phase: test loss: 5.032550377895089e-08, acc: 1.0\n",
      "Epoch: 137 Phase: train loss: 3.92026856523548e-09, acc: 1.0\n",
      "Epoch: 137 Phase: test loss: 2.4630015566100523e-10, acc: 1.0\n",
      "Epoch: 138 Phase: train loss: 1.990924442754593e-09, acc: 1.0\n",
      "Epoch: 138 Phase: test loss: 1.642001135610935e-10, acc: 1.0\n",
      "Epoch: 139 Phase: train loss: 1.662525125831778e-09, acc: 1.0\n",
      "Epoch: 139 Phase: test loss: 4.359386641204513e-08, acc: 1.0\n",
      "Epoch: 140 Phase: train loss: 1.1904502250820503e-09, acc: 1.0\n",
      "Epoch: 140 Phase: test loss: 2.0524991880571434e-09, acc: 1.0\n",
      "Epoch: 141 Phase: train loss: 5.541753465671031e-10, acc: 1.0\n",
      "Epoch: 141 Phase: test loss: 2.463001703416403e-10, acc: 1.0\n",
      "Epoch: 142 Phase: train loss: 2.031972155274689e-09, acc: 1.0\n",
      "Epoch: 142 Phase: test loss: 2.1345992595183253e-09, acc: 1.0\n",
      "Epoch: 143 Phase: train loss: 1.8472497287553058e-09, acc: 1.0\n",
      "Epoch: 143 Phase: test loss: 3.7108240682280004e-08, acc: 1.0\n",
      "Epoch: 144 Phase: train loss: 1.5393755875146131e-09, acc: 1.0\n",
      "Epoch: 144 Phase: test loss: 4.1050025454146373e-10, acc: 1.0\n",
      "Epoch: 145 Phase: train loss: 1.929350603981256e-09, acc: 1.0\n",
      "Epoch: 145 Phase: test loss: 3.628735910102151e-08, acc: 1.0\n",
      "Epoch: 146 Phase: train loss: 1.8677747135891724e-09, acc: 1.0\n",
      "Epoch: 146 Phase: test loss: 4.926003113220105e-10, acc: 1.0\n",
      "Epoch: 147 Phase: train loss: 2.99663934437938e-09, acc: 1.0\n",
      "Epoch: 147 Phase: test loss: 1.970399234041042e-09, acc: 1.0\n",
      "Epoch: 148 Phase: train loss: 1.6214736036868897e-09, acc: 1.0\n",
      "Epoch: 148 Phase: test loss: 1.970399234041042e-09, acc: 1.0\n",
      "Epoch: 149 Phase: train loss: 1.8267256027385886e-09, acc: 1.0\n",
      "Epoch: 149 Phase: test loss: 2.0524992761409537e-09, acc: 1.0\n",
      "Epoch: 150 Phase: train loss: 2.0257287982954398e-08, acc: 1.0\n",
      "Epoch: 150 Phase: test loss: 2.463001703416403e-10, acc: 1.0\n",
      "Epoch: 151 Phase: train loss: 7.79950436650749e-10, acc: 1.0\n",
      "Epoch: 151 Phase: test loss: 1.642001135610935e-10, acc: 1.0\n",
      "Epoch: 152 Phase: train loss: 4.35126517737249e-09, acc: 1.0\n",
      "Epoch: 152 Phase: test loss: 1.2315000149120042e-09, acc: 1.0\n",
      "Epoch: 153 Phase: train loss: 2.6066716535496204e-09, acc: 1.0\n",
      "Epoch: 153 Phase: test loss: 1.642001135610935e-10, acc: 1.0\n",
      "Epoch: 154 Phase: train loss: 6.773252225388739e-10, acc: 1.0\n",
      "Epoch: 154 Phase: test loss: 1.313600057011916e-09, acc: 1.0\n",
      "Epoch: 155 Phase: train loss: 1.785674862337516e-09, acc: 1.0\n",
      "Epoch: 155 Phase: test loss: 1.3136000716925511e-09, acc: 1.0\n",
      "Epoch: 156 Phase: train loss: 1.7240992178460696e-09, acc: 1.0\n",
      "Epoch: 156 Phase: test loss: 2.463001703416403e-10, acc: 1.0\n",
      "Epoch: 157 Phase: train loss: 8.210004466902286e-10, acc: 1.0\n",
      "Epoch: 157 Phase: test loss: 2.463001703416403e-10, acc: 1.0\n",
      "Epoch: 158 Phase: train loss: 1.0262501335419092e-09, acc: 1.0\n",
      "Epoch: 158 Phase: test loss: 1.2315000149120042e-09, acc: 1.0\n",
      "Epoch: 159 Phase: train loss: 3.879220731600144e-09, acc: 1.0\n",
      "Epoch: 159 Phase: test loss: 2.2330855357692547e-08, acc: 1.0\n",
      "Epoch: 160 Phase: train loss: 8.825751369403977e-10, acc: 1.0\n",
      "Epoch: 160 Phase: test loss: 2.2741380628036113e-08, acc: 1.0\n",
      "Epoch: 161 Phase: train loss: 1.9498734564528834e-09, acc: 1.0\n",
      "Epoch: 161 Phase: test loss: 1.149400148979713e-09, acc: 1.0\n",
      "Epoch: 162 Phase: train loss: 3.0171667405075516e-09, acc: 1.0\n",
      "Epoch: 162 Phase: test loss: 2.142778349247057e-08, acc: 1.0\n",
      "Epoch: 163 Phase: train loss: 1.2540461187388832e-08, acc: 1.0\n",
      "Epoch: 163 Phase: test loss: 2.1591977352081138e-08, acc: 1.0\n",
      "Epoch: 164 Phase: train loss: 5.952253235751538e-10, acc: 1.0\n",
      "Epoch: 164 Phase: test loss: 2.4547488111992163e-08, acc: 1.0\n",
      "Epoch: 165 Phase: train loss: 1.8472509839496012e-10, acc: 1.0\n",
      "Epoch: 165 Phase: test loss: 9.031003309733136e-10, acc: 1.0\n",
      "Epoch: 166 Phase: train loss: 1.2930753584504327e-09, acc: 1.0\n",
      "Epoch: 166 Phase: test loss: 8.210005678054675e-11, acc: 1.0\n",
      "Epoch: 167 Phase: train loss: 4.310252393753303e-10, acc: 1.0\n",
      "Epoch: 167 Phase: test loss: 8.210005678054675e-11, acc: 1.0\n",
      "Epoch: 168 Phase: train loss: 1.1494000352047915e-09, acc: 1.0\n",
      "Epoch: 168 Phase: test loss: 2.463001703416403e-10, acc: 1.0\n",
      "Epoch: 169 Phase: train loss: 1.0878243099698522e-09, acc: 1.0\n",
      "Epoch: 169 Phase: test loss: 1.642001135610935e-10, acc: 1.0\n",
      "Epoch: 170 Phase: train loss: 4.269166504241795e-09, acc: 1.0\n",
      "Epoch: 170 Phase: test loss: 8.210005678054675e-11, acc: 1.0\n",
      "Epoch: 171 Phase: train loss: 1.5804238799197934e-09, acc: 1.0\n",
      "Epoch: 171 Phase: test loss: 8.210005678054675e-11, acc: 1.0\n",
      "Epoch: 172 Phase: train loss: 1.5188468848099235e-09, acc: 1.0\n",
      "Epoch: 172 Phase: test loss: 8.210005678054675e-11, acc: 1.0\n",
      "Epoch: 173 Phase: train loss: 1.6625232283596997e-09, acc: 1.0\n",
      "Epoch: 173 Phase: test loss: 1.5516747086484586e-08, acc: 1.0\n",
      "Epoch: 174 Phase: train loss: 1.990923506864109e-09, acc: 1.0\n",
      "Epoch: 174 Phase: test loss: 1.642001135610935e-10, acc: 1.0\n",
      "Epoch: 175 Phase: train loss: 8.004752563274714e-10, acc: 1.0\n",
      "Epoch: 175 Phase: test loss: 4.926002232382002e-10, acc: 1.0\n",
      "Epoch: 176 Phase: train loss: 1.6009457010768097e-09, acc: 1.0\n",
      "Epoch: 176 Phase: test loss: 8.210005678054675e-11, acc: 1.0\n",
      "Epoch: 177 Phase: train loss: 2.6682516618598315e-10, acc: 1.0\n",
      "Epoch: 177 Phase: test loss: 4.926002232382002e-10, acc: 1.0\n",
      "Epoch: 178 Phase: train loss: 4.926001168035962e-10, acc: 1.0\n",
      "Epoch: 178 Phase: test loss: 1.0016134551695976e-08, acc: 1.0\n",
      "Epoch: 179 Phase: train loss: 5.541752841744041e-10, acc: 1.0\n",
      "Epoch: 179 Phase: test loss: 4.926002232382002e-10, acc: 1.0\n",
      "Epoch: 180 Phase: train loss: 1.990922948999978e-09, acc: 1.0\n",
      "Epoch: 180 Phase: test loss: 0.0, acc: 1.0\n",
      "Epoch: 181 Phase: train loss: 3.694499618997597e-10, acc: 1.0\n",
      "Epoch: 181 Phase: test loss: 0.0, acc: 1.0\n",
      "Epoch: 182 Phase: train loss: 7.594252279371981e-10, acc: 1.0\n",
      "Epoch: 182 Phase: test loss: 0.0, acc: 1.0\n",
      "Epoch: 183 Phase: train loss: 4.3102505219723364e-10, acc: 1.0\n",
      "Epoch: 183 Phase: test loss: 7.224768001487823e-09, acc: 1.0\n",
      "Epoch: 184 Phase: train loss: 7.799502091009058e-10, acc: 1.0\n",
      "Epoch: 184 Phase: test loss: 4.105001811382885e-10, acc: 1.0\n",
      "Epoch: 185 Phase: train loss: 1.2930737252297849e-09, acc: 1.0\n",
      "Epoch: 185 Phase: test loss: 0.0, acc: 1.0\n",
      "Epoch: 186 Phase: train loss: 2.4630015566100523e-10, acc: 1.0\n",
      "Epoch: 186 Phase: test loss: 4.105001811382885e-10, acc: 1.0\n",
      "Epoch: 187 Phase: train loss: 1.64200106220776e-10, acc: 1.0\n",
      "Epoch: 187 Phase: test loss: 0.0, acc: 1.0\n",
      "Epoch: 188 Phase: train loss: 2.8735018405127863e-10, acc: 1.0\n",
      "Epoch: 188 Phase: test loss: 0.0, acc: 1.0\n",
      "Epoch: 189 Phase: train loss: 8.210005678054675e-11, acc: 1.0\n",
      "Epoch: 189 Phase: test loss: 0.0, acc: 1.0\n",
      "Epoch: 190 Phase: train loss: 8.415253250894912e-10, acc: 1.0\n",
      "Epoch: 190 Phase: test loss: 6.239579974828663e-09, acc: 1.0\n",
      "Epoch: 191 Phase: train loss: 1.2315008150066138e-10, acc: 1.0\n",
      "Epoch: 191 Phase: test loss: 0.0, acc: 1.0\n",
      "Epoch: 192 Phase: train loss: 1.8472512041591269e-10, acc: 1.0\n",
      "Epoch: 192 Phase: test loss: 4.105001811382885e-10, acc: 1.0\n",
      "Epoch: 193 Phase: train loss: 3.6945019678992025e-10, acc: 1.0\n",
      "Epoch: 193 Phase: test loss: 4.105001811382885e-10, acc: 1.0\n",
      "Epoch: 194 Phase: train loss: 1.0262506730552469e-10, acc: 1.0\n",
      "Epoch: 194 Phase: test loss: 0.0, acc: 1.0\n",
      "Epoch: 195 Phase: train loss: 1.5188478610721534e-09, acc: 1.0\n",
      "Epoch: 195 Phase: test loss: 5.336486499711911e-09, acc: 1.0\n",
      "Epoch: 196 Phase: train loss: 1.785673629164173e-09, acc: 1.0\n",
      "Epoch: 196 Phase: test loss: 4.4333887672109995e-09, acc: 1.0\n",
      "Epoch: 197 Phase: train loss: 1.6009467067003097e-09, acc: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 197 Phase: test loss: 3.284001683996469e-10, acc: 1.0\n",
      "Epoch: 198 Phase: train loss: 3.489251238722434e-10, acc: 1.0\n",
      "Epoch: 198 Phase: test loss: 0.0, acc: 1.0\n",
      "Epoch: 199 Phase: train loss: 4.105000490125732e-10, acc: 1.0\n",
      "Epoch: 199 Phase: test loss: 4.351289488504109e-09, acc: 1.0\n",
      "Epoch: 200 Phase: train loss: 5.541751447083713e-10, acc: 1.0\n",
      "Epoch: 200 Phase: test loss: 4.515490629709656e-09, acc: 1.0\n",
      "Epoch: 201 Phase: train loss: 4.515499526174488e-10, acc: 1.0\n",
      "Epoch: 201 Phase: test loss: 0.0, acc: 1.0\n",
      "Epoch: 202 Phase: train loss: 1.8472511307559516e-10, acc: 1.0\n",
      "Epoch: 202 Phase: test loss: 4.187090461310009e-09, acc: 1.0\n",
      "Epoch: 203 Phase: train loss: 1.790707822171535e-07, acc: 1.0\n",
      "Epoch: 203 Phase: test loss: 2.113705293629382e-06, acc: 1.0\n",
      "Epoch: 204 Phase: train loss: 1.0057167179863572e-08, acc: 1.0\n",
      "Epoch: 204 Phase: test loss: 0.0, acc: 1.0\n",
      "Epoch: 205 Phase: train loss: 9.030983490875837e-10, acc: 1.0\n",
      "Epoch: 205 Phase: test loss: 0.0, acc: 1.0\n",
      "Epoch: 206 Phase: train loss: 1.4367504431357543e-10, acc: 1.0\n",
      "Epoch: 206 Phase: test loss: 0.0, acc: 1.0\n",
      "Epoch: 207 Phase: train loss: 3.0787500372800105e-10, acc: 1.0\n",
      "Epoch: 207 Phase: test loss: 0.0, acc: 1.0\n",
      "Epoch: 208 Phase: train loss: 0.0, acc: 1.0\n",
      "Epoch: 208 Phase: test loss: 0.0, acc: 1.0\n",
      "Epoch: 209 Phase: train loss: 1.642000805296647e-10, acc: 1.0\n",
      "Epoch: 209 Phase: test loss: 0.0, acc: 1.0\n",
      "Epoch: 210 Phase: train loss: 2.052501419513669e-11, acc: 1.0\n",
      "Epoch: 210 Phase: test loss: 3.80933775616067e-08, acc: 1.0\n",
      "Epoch: 211 Phase: train loss: 2.052501419513669e-11, acc: 1.0\n",
      "Epoch: 211 Phase: test loss: 0.0, acc: 1.0\n",
      "Epoch: 212 Phase: train loss: 2.052501419513669e-11, acc: 1.0\n",
      "Epoch: 212 Phase: test loss: 0.0, acc: 1.0\n",
      "Epoch: 213 Phase: train loss: 0.0, acc: 1.0\n",
      "Epoch: 213 Phase: test loss: 3.7108229436913564e-08, acc: 1.0\n",
      "Epoch: 214 Phase: train loss: 1.8882912387070994e-09, acc: 1.0\n",
      "Epoch: 214 Phase: test loss: 0.0, acc: 1.0\n",
      "Epoch: 215 Phase: train loss: 1.642000805296647e-10, acc: 1.0\n",
      "Epoch: 215 Phase: test loss: 0.0, acc: 1.0\n",
      "Epoch: 216 Phase: train loss: 4.5154994894729003e-10, acc: 1.0\n",
      "Epoch: 216 Phase: test loss: 0.0, acc: 1.0\n",
      "Epoch: 217 Phase: train loss: 2.052501419513669e-11, acc: 1.0\n",
      "Epoch: 217 Phase: test loss: 0.0, acc: 1.0\n",
      "Epoch: 218 Phase: train loss: 4.105002839027338e-11, acc: 1.0\n",
      "Epoch: 218 Phase: test loss: 0.0, acc: 1.0\n",
      "Epoch: 219 Phase: train loss: 2.052501419513669e-11, acc: 1.0\n",
      "Epoch: 219 Phase: test loss: 0.0, acc: 1.0\n",
      "Epoch: 220 Phase: train loss: 2.052501419513669e-11, acc: 1.0\n",
      "Epoch: 220 Phase: test loss: 0.0, acc: 1.0\n",
      "Epoch: 221 Phase: train loss: 0.0, acc: 1.0\n",
      "Epoch: 221 Phase: test loss: 0.0, acc: 1.0\n",
      "Epoch: 222 Phase: train loss: 1.4367508101516303e-10, acc: 1.0\n",
      "Epoch: 222 Phase: test loss: 3.776592423544436e-09, acc: 1.0\n",
      "Epoch: 223 Phase: train loss: 2.052501419513669e-11, acc: 1.0\n",
      "Epoch: 223 Phase: test loss: 0.0, acc: 1.0\n",
      "Epoch: 224 Phase: train loss: 4.1050024720114617e-11, acc: 1.0\n",
      "Epoch: 224 Phase: test loss: 3.5302936478631238e-09, acc: 1.0\n",
      "Epoch: 225 Phase: train loss: 0.0, acc: 1.0\n",
      "Epoch: 225 Phase: test loss: 0.0, acc: 1.0\n",
      "Epoch: 226 Phase: train loss: 1.0262504528457213e-10, acc: 1.0\n",
      "Epoch: 226 Phase: test loss: 0.0, acc: 1.0\n",
      "Epoch: 227 Phase: train loss: 0.0, acc: 1.0\n",
      "Epoch: 227 Phase: test loss: 0.0, acc: 1.0\n",
      "Epoch: 228 Phase: train loss: 0.0, acc: 1.0\n",
      "Epoch: 228 Phase: test loss: 0.0, acc: 1.0\n",
      "Epoch: 229 Phase: train loss: 0.0, acc: 1.0\n",
      "Epoch: 229 Phase: test loss: 0.0, acc: 1.0\n",
      "Epoch: 230 Phase: train loss: 0.0, acc: 1.0\n",
      "Epoch: 230 Phase: test loss: 3.3660943857788624e-09, acc: 1.0\n",
      "Epoch: 231 Phase: train loss: 2.052501419513669e-11, acc: 1.0\n",
      "Epoch: 231 Phase: test loss: 0.0, acc: 1.0\n",
      "Epoch: 232 Phase: train loss: 2.052501419513669e-11, acc: 1.0\n",
      "Epoch: 232 Phase: test loss: 3.2839946372916514e-09, acc: 1.0\n",
      "Epoch: 233 Phase: train loss: 2.052501419513669e-11, acc: 1.0\n",
      "Epoch: 233 Phase: test loss: 0.0, acc: 1.0\n",
      "Epoch: 234 Phase: train loss: 0.0, acc: 1.0\n",
      "Epoch: 234 Phase: test loss: 3.2018948888044404e-09, acc: 1.0\n",
      "Epoch: 235 Phase: train loss: 0.0, acc: 1.0\n",
      "Epoch: 235 Phase: test loss: 3.2018948888044404e-09, acc: 1.0\n",
      "Epoch: 236 Phase: train loss: 0.0, acc: 1.0\n",
      "Epoch: 236 Phase: test loss: 0.0, acc: 1.0\n",
      "Epoch: 237 Phase: train loss: 2.052501419513669e-11, acc: 1.0\n",
      "Epoch: 237 Phase: test loss: 0.0, acc: 1.0\n",
      "Epoch: 238 Phase: train loss: 0.0, acc: 1.0\n",
      "Epoch: 238 Phase: test loss: 3.1197951403172293e-09, acc: 1.0\n",
      "Epoch: 239 Phase: train loss: 2.052501419513669e-11, acc: 1.0\n",
      "Epoch: 239 Phase: test loss: 0.0, acc: 1.0\n",
      "Epoch: 240 Phase: train loss: 2.052501419513669e-11, acc: 1.0\n",
      "Epoch: 240 Phase: test loss: 3.1197951403172293e-09, acc: 1.0\n",
      "Epoch: 241 Phase: train loss: 0.0, acc: 1.0\n",
      "Epoch: 241 Phase: test loss: 0.0, acc: 1.0\n",
      "Epoch: 242 Phase: train loss: 0.0, acc: 1.0\n",
      "Epoch: 242 Phase: test loss: 3.037695626720179e-09, acc: 1.0\n",
      "Epoch: 243 Phase: train loss: 0.0, acc: 1.0\n",
      "Epoch: 243 Phase: test loss: 0.0, acc: 1.0\n",
      "Epoch: 244 Phase: train loss: 4.105002839027338e-11, acc: 1.0\n",
      "Epoch: 244 Phase: test loss: 0.0, acc: 1.0\n",
      "Epoch: 245 Phase: train loss: 5.952244097056228e-10, acc: 1.0\n",
      "Epoch: 245 Phase: test loss: 0.0, acc: 1.0\n",
      "Epoch: 246 Phase: train loss: 0.0, acc: 1.0\n",
      "Epoch: 246 Phase: test loss: 5.747001772543017e-10, acc: 1.0\n",
      "Epoch: 247 Phase: train loss: 2.052501419513669e-11, acc: 1.0\n",
      "Epoch: 247 Phase: test loss: 5.747001772543017e-10, acc: 1.0\n",
      "Epoch: 248 Phase: train loss: 2.052501419513669e-11, acc: 1.0\n",
      "Epoch: 248 Phase: test loss: 0.0, acc: 1.0\n",
      "Epoch: 249 Phase: train loss: 0.0, acc: 1.0\n",
      "Epoch: 249 Phase: test loss: 5.747001772543017e-10, acc: 1.0\n",
      "Epoch: 250 Phase: train loss: 0.0, acc: 1.0\n",
      "Epoch: 250 Phase: test loss: 5.747001772543017e-10, acc: 1.0\n",
      "Epoch: 251 Phase: train loss: 2.052501419513669e-11, acc: 1.0\n",
      "Epoch: 251 Phase: test loss: 0.0, acc: 1.0\n",
      "Epoch: 252 Phase: train loss: 2.052501419513669e-11, acc: 1.0\n",
      "Epoch: 252 Phase: test loss: 0.0, acc: 1.0\n",
      "Epoch: 253 Phase: train loss: 0.0, acc: 1.0\n",
      "Epoch: 253 Phase: test loss: 0.0, acc: 1.0\n",
      "Epoch: 254 Phase: train loss: 2.052501419513669e-11, acc: 1.0\n",
      "Epoch: 254 Phase: test loss: 0.0, acc: 1.0\n",
      "Epoch: 255 Phase: train loss: 0.0, acc: 1.0\n",
      "Epoch: 255 Phase: test loss: 0.0, acc: 1.0\n",
      "Epoch: 256 Phase: train loss: 0.0, acc: 1.0\n",
      "Epoch: 256 Phase: test loss: 4.926002232382002e-10, acc: 1.0\n",
      "Epoch: 257 Phase: train loss: 0.0, acc: 1.0\n",
      "Epoch: 257 Phase: test loss: 4.926002232382002e-10, acc: 1.0\n",
      "Epoch: 258 Phase: train loss: 0.0, acc: 1.0\n",
      "Epoch: 258 Phase: test loss: 0.0, acc: 1.0\n",
      "Epoch: 259 Phase: train loss: 0.0, acc: 1.0\n",
      "Epoch: 259 Phase: test loss: 0.0, acc: 1.0\n",
      "Epoch: 260 Phase: train loss: 4.1050024720114617e-11, acc: 1.0\n",
      "Epoch: 260 Phase: test loss: 0.0, acc: 1.0\n",
      "Epoch: 261 Phase: train loss: 8.210004944022923e-11, acc: 1.0\n",
      "Epoch: 261 Phase: test loss: 0.0, acc: 1.0\n",
      "Epoch: 262 Phase: train loss: 3.4892501376748065e-10, acc: 1.0\n",
      "Epoch: 262 Phase: test loss: 4.105001811382885e-10, acc: 1.0\n",
      "Epoch: 263 Phase: train loss: 0.0, acc: 1.0\n",
      "Epoch: 263 Phase: test loss: 0.0, acc: 1.0\n",
      "Epoch: 264 Phase: train loss: 8.620484748439782e-10, acc: 1.0\n",
      "Epoch: 264 Phase: test loss: 0.0, acc: 1.0\n",
      "Epoch: 265 Phase: train loss: 2.052501419513669e-11, acc: 1.0\n",
      "Epoch: 265 Phase: test loss: 0.0, acc: 1.0\n",
      "Epoch: 266 Phase: train loss: 2.052501419513669e-11, acc: 1.0\n",
      "Epoch: 266 Phase: test loss: 0.0, acc: 1.0\n",
      "Epoch: 267 Phase: train loss: 0.0, acc: 1.0\n",
      "Epoch: 267 Phase: test loss: 0.0, acc: 1.0\n",
      "Epoch: 268 Phase: train loss: 0.0, acc: 1.0\n",
      "Epoch: 268 Phase: test loss: 0.0, acc: 1.0\n",
      "Epoch: 269 Phase: train loss: 2.052501419513669e-11, acc: 1.0\n",
      "Epoch: 269 Phase: test loss: 3.284001683996469e-10, acc: 1.0\n",
      "Epoch: 270 Phase: train loss: 2.052501419513669e-11, acc: 1.0\n",
      "Epoch: 270 Phase: test loss: 0.0, acc: 1.0\n",
      "Epoch: 271 Phase: train loss: 0.0, acc: 1.0\n",
      "Epoch: 271 Phase: test loss: 3.284001683996469e-10, acc: 1.0\n",
      "Epoch: 272 Phase: train loss: 0.0, acc: 1.0\n",
      "Epoch: 272 Phase: test loss: 3.284001683996469e-10, acc: 1.0\n",
      "Epoch: 273 Phase: train loss: 0.0, acc: 1.0\n",
      "Epoch: 273 Phase: test loss: 0.0, acc: 1.0\n",
      "Epoch: 274 Phase: train loss: 0.0, acc: 1.0\n",
      "Epoch: 274 Phase: test loss: 0.0, acc: 1.0\n",
      "Epoch: 275 Phase: train loss: 0.0, acc: 1.0\n",
      "Epoch: 275 Phase: test loss: 0.0, acc: 1.0\n",
      "Epoch: 276 Phase: train loss: 6.157504258541007e-11, acc: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 276 Phase: test loss: 0.0, acc: 1.0\n",
      "Epoch: 277 Phase: train loss: 1.2315005580955005e-10, acc: 1.0\n",
      "Epoch: 277 Phase: test loss: 2.463001409803702e-10, acc: 1.0\n",
      "Epoch: 278 Phase: train loss: 0.0, acc: 1.0\n",
      "Epoch: 278 Phase: test loss: 0.0, acc: 1.0\n",
      "Epoch: 279 Phase: train loss: 0.0, acc: 1.0\n",
      "Epoch: 279 Phase: test loss: 2.463001409803702e-10, acc: 1.0\n",
      "Epoch: 280 Phase: train loss: 0.0, acc: 1.0\n",
      "Epoch: 280 Phase: test loss: 0.0, acc: 1.0\n",
      "Epoch: 281 Phase: train loss: 1.4367504431357543e-10, acc: 1.0\n",
      "Epoch: 281 Phase: test loss: 2.463001409803702e-10, acc: 1.0\n",
      "Epoch: 282 Phase: train loss: 0.0, acc: 1.0\n",
      "Epoch: 282 Phase: test loss: 2.463001409803702e-10, acc: 1.0\n",
      "Epoch: 283 Phase: train loss: 0.0, acc: 1.0\n",
      "Epoch: 283 Phase: test loss: 0.0, acc: 1.0\n",
      "Epoch: 284 Phase: train loss: 2.052501419513669e-11, acc: 1.0\n",
      "Epoch: 284 Phase: test loss: 0.0, acc: 1.0\n",
      "Epoch: 285 Phase: train loss: 0.0, acc: 1.0\n",
      "Epoch: 285 Phase: test loss: 0.0, acc: 1.0\n",
      "Epoch: 286 Phase: train loss: 2.052501419513669e-11, acc: 1.0\n",
      "Epoch: 286 Phase: test loss: 0.0, acc: 1.0\n",
      "Epoch: 287 Phase: train loss: 0.0, acc: 1.0\n",
      "Epoch: 287 Phase: test loss: 2.463001409803702e-10, acc: 1.0\n",
      "Epoch: 288 Phase: train loss: 4.1050024720114617e-11, acc: 1.0\n",
      "Epoch: 288 Phase: test loss: 0.0, acc: 1.0\n",
      "Epoch: 289 Phase: train loss: 0.0, acc: 1.0\n",
      "Epoch: 289 Phase: test loss: 0.0, acc: 1.0\n",
      "Epoch: 290 Phase: train loss: 2.052501419513669e-11, acc: 1.0\n",
      "Epoch: 290 Phase: test loss: 2.463001409803702e-10, acc: 1.0\n",
      "Epoch: 291 Phase: train loss: 0.0, acc: 1.0\n",
      "Epoch: 291 Phase: test loss: 0.0, acc: 1.0\n",
      "Epoch: 292 Phase: train loss: 0.0, acc: 1.0\n",
      "Epoch: 292 Phase: test loss: 0.0, acc: 1.0\n",
      "Epoch: 293 Phase: train loss: 1.4367504431357543e-10, acc: 1.0\n",
      "Epoch: 293 Phase: test loss: 0.0, acc: 1.0\n",
      "Epoch: 294 Phase: train loss: 6.157503524509255e-11, acc: 1.0\n",
      "Epoch: 294 Phase: test loss: 1.6420009888045847e-10, acc: 1.0\n",
      "Epoch: 295 Phase: train loss: 0.0, acc: 1.0\n",
      "Epoch: 295 Phase: test loss: 1.6420009888045847e-10, acc: 1.0\n",
      "Epoch: 296 Phase: train loss: 0.0, acc: 1.0\n",
      "Epoch: 296 Phase: test loss: 0.0, acc: 1.0\n",
      "Epoch: 297 Phase: train loss: 0.0, acc: 1.0\n",
      "Epoch: 297 Phase: test loss: 0.0, acc: 1.0\n",
      "Epoch: 298 Phase: train loss: 0.0, acc: 1.0\n",
      "Epoch: 298 Phase: test loss: 0.0, acc: 1.0\n",
      "Epoch: 299 Phase: train loss: 0.0, acc: 1.0\n",
      "Epoch: 299 Phase: test loss: 0.0, acc: 1.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms, models\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.autograd import Variable\n",
    "import torch.utils.data as Data\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from data import microPlankton\n",
    "import cv2 as cv\n",
    "from torchvision import transforms\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, train_test_split\n",
    "from dataTransformNew import dataTran\n",
    "import time\n",
    "import winsound\n",
    "\n",
    "batchSize = 96  # 11111111111\n",
    "numClasses = 2\n",
    "numEpoch = 300\n",
    "testSize = 0.2  # 11111111111\n",
    "learningRate = 0.0001  # 11111111111\n",
    "dropNum = 0.5\n",
    "momentum = 0.9\n",
    "\n",
    "featureExtract = True\n",
    "usePretrained = False\n",
    "dropout = True\n",
    "mono = True\n",
    "savePreAndRealFlag = True\n",
    "\n",
    "samplePath = 'C:/automated_classification/holographic_plankton_classification-main/samples/'\n",
    "inputSize = 128\n",
    "imgType = '.tif'\n",
    "txtFile = \"trainlist_new_v2_withoutdecoy.txt\"  # 11111111111\n",
    "#txtFile = \"new_trainlist_SS_v5.txt\" \n",
    "#modelName = \"shufflenet_v2_x1_5\" original one used for diatoms\n",
    "#modelName = \"vgg19\"\n",
    "#modelnameslist = [\"shufflenet_v2_x2_0\", \"shufflenet_v2_x1_5\", \"mobilenet_v2\", \"resnet18\"]\n",
    "#modelnameslist = [\"vgg16\", \"vgg19_bn\", \"resnet50\",\"vgg19\"]\n",
    "#modelnameslist = [\"inception_v3\", \"googlenet\", \"densenet121\", \"densenet161\"]\n",
    "modelnameslist = [\"vgg19\"]\n",
    "#batchSizelist = [64] \n",
    "for modelName in modelnameslist:\n",
    "#for batchSize in batchSizelist:\n",
    "    for lrindex in [1]:\n",
    "        if lrindex == 1:\n",
    "            #learningRate = 0.0001\n",
    "            #featureExtract = True\n",
    "            batchSize = 96\n",
    "            #numEpoch = 100\n",
    "            #ksize = (3,3)\n",
    "            #ssize = (1,1)\n",
    "            #numClasses = 3\n",
    "            #txtFile = \"extra_set_train_v1.txt\"\n",
    "            \n",
    "        #if lrindex == 2:\n",
    "            #learningRate = 0.0001\n",
    "            #featureExtract = True\n",
    "            #numEpoch = 200\n",
    "            #batchSize = 96\n",
    "            #ksize = (3,3)\n",
    "            #ssize = (2,2)\n",
    "            #numClasses = 3\n",
    "            #txtFile = \"extra_set_train_v2.txt\"\n",
    "            \n",
    "        #if lrindex == 3:\n",
    "            #learningRate = 0.00001\n",
    "            #featureExtract = False\n",
    "            #numEpoch = 300\n",
    "            #batchSize = 96\n",
    "            #ksize = (2,2)\n",
    "            #ssize = (1,1)\n",
    "            #numClasses = 2\n",
    "            #txtFile = \"test_new_set_v3.txt\"\n",
    "        \n",
    "        #if lrindex == 4:\n",
    "            #learningRate = 0.0001\n",
    "            #featureExtract = False\n",
    "            #numEpoch = 200\n",
    "            #momentum = 0.5\n",
    "            #batchSize = 160\n",
    "            #ksize = (2,2)\n",
    "            #ssize = (1,1)\n",
    "            #numClasses = 2\n",
    "            #txtFile = \"new_train_v4.txt\"\n",
    "        #ksize = (3,3)\n",
    "        #ssize = (1,1)\n",
    "    \n",
    "        #modelName = \"vgg19\" #mC:/automate playing with it\n",
    "        filePath = \"C:/automated_classification/holographic_plankton_classification-main/\"\n",
    "        savePath = 'C:/automated_classification/holographic_plankton_classification-main/modelGen/' + modelName + '_'\n",
    "\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "        # --------------------find the input size for padding START\n",
    "        def findMax(imgPath, fileType):\n",
    "            maxPixel = 0\n",
    "            for mainPath, dirs, file in os.walk(imgPath, topdown=False):\n",
    "                for subFolderName in dirs:  # read subfolder\n",
    "                    pathNow = os.path.join(mainPath, subFolderName)\n",
    "                    for subMainPath, subDirs, subFile in os.walk(pathNow, topdown=False):  # find data file\n",
    "                        for dataFile in subFile:  # read data file\n",
    "                            if os.path.splitext(dataFile)[1] == fileType:  # find data with specific suffix\n",
    "                                img = cv.imread(subMainPath + '/' + dataFile)\n",
    "                                try:\n",
    "                                    tempPixel = max(img.shape)\n",
    "                                except:\n",
    "                                    print(dataFile)\n",
    "                                maxPixel = max(tempPixel, maxPixel)\n",
    "            return maxPixel\n",
    "\n",
    "\n",
    "        # inputSize = findMax(samplePath, imgType)\n",
    "        # --------------------find the input size for padding END\n",
    "\n",
    "        dataTransforms = {\n",
    "            \"trainData\": transforms.Compose([\n",
    "                # transforms.Resize(inputSize),\n",
    "                # transforms.RandomCrop(size=inputSize, pad_if_needed=True, padding_mode='constant'),\n",
    "                transforms.RandomHorizontalFlip(p=0.5),  # horizontal flip and 0.5 is the position\n",
    "                transforms.RandomVerticalFlip(p=0.5),\n",
    "                torchvision.transforms.RandomRotation(45, resample=False, expand=False,\n",
    "                                                      center=None),\n",
    "                transforms.ToTensor(),\n",
    "                # transforms.Normalize([],[])\n",
    "            ]),\n",
    "            \"testData\": transforms.Compose([\n",
    "                # transforms.Resize(inputSize),\n",
    "                # transforms.RandomCrop(size=inputSize, pad_if_needed=True, padding_mode='constant'),\n",
    "                transforms.RandomHorizontalFlip(p=0.5),  # horizontal flip and 0.5 is the position\n",
    "                transforms.RandomVerticalFlip(p=0.5),\n",
    "                transforms.ToTensor(),\n",
    "                # transforms.Normalize([],[])\n",
    "            ])\n",
    "        }\n",
    "\n",
    "        imagDataset = microPlankton(root=filePath, dataTXT=txtFile, )\n",
    "        train_set, test_set = train_test_split(imagDataset, test_size=testSize,random_state=42)\n",
    "        trainSet = dataTran(inputSize,train_set, transform=dataTransforms[\"trainData\"])\n",
    "        testSet = dataTran(inputSize,test_set, transform=dataTransforms[\"testData\"])\n",
    "        testDataloader = torch.utils.data.DataLoader(testSet, batch_size=batchSize, shuffle=True, num_workers=0,\n",
    "                                                     pin_memory=True)\n",
    "        trainDataloader = torch.utils.data.DataLoader(trainSet, batch_size=batchSize, shuffle=True, num_workers=0,\n",
    "                                                      pin_memory=True)\n",
    "\n",
    "\n",
    "        # -------------copy from resnet\n",
    "\n",
    "\n",
    "        # --------------------Show imgs in the dataset Start\n",
    "        # imgs = next(iter(trainDataloader['trainData']))[0]\n",
    "        # unloader = transforms.ToPILImage()\n",
    "        #\n",
    "        # def imgShow( tensor, title):\n",
    "        #     img = tensor.cpu().clone()\n",
    "        #     img = img.squeeze(0)\n",
    "        #     img = unloader(img)\n",
    "        #     plt.imshow(img, cmap=\"gray\")\n",
    "        #     if title is not None:\n",
    "        #         plt.title(title)\n",
    "        #     plt.pause(0.001)\n",
    "        #\n",
    "        # # print(imgs.shape)\n",
    "        # plt.figure()\n",
    "        # imgShow(imgs[3],title=\"img\")\n",
    "\n",
    "        # --------------------Show imgs in the dataset End\n",
    "\n",
    "\n",
    "        def setParameterRequiresGrad(model, featureExtract):\n",
    "            if featureExtract:\n",
    "                for param in model.parameters():\n",
    "                    param.requires_grad = False\n",
    "\n",
    "\n",
    "        def initialModel(modelName, numClasses, featureExtract, usePretrained):\n",
    "            if modelName == \"vgg19\":\n",
    "                modelUse = models.vgg19(pretrained=usePretrained)\n",
    "                setParameterRequiresGrad(modelUse, featureExtract)\n",
    "                if mono:\n",
    "                    modelUse.features._modules['0'] = nn.Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "                modelUse.classifier._modules['6'] = nn.Linear(in_features=4096, out_features=numClasses, bias=True)\n",
    "                #if dropout:\n",
    "                 #   modelUse.classifier._modules['6'] = nn.Sequential(nn.Dropout(dropNum),\n",
    "                  #                                                    nn.Linear(in_features=4096, out_features=numClasses, bias=True))\n",
    "               # else:    \n",
    "                 #   modelUse.classifier._modules['6'] = nn.Linear(in_features=4096, out_features=numClasses, bias=True)\n",
    "\n",
    "            \n",
    "            elif modelName == \"vgg19_bn\":\n",
    "                modelUse = models.vgg19_bn(pretrained=usePretrained)\n",
    "                setParameterRequiresGrad(modelUse, featureExtract)\n",
    "                if mono:\n",
    "                    modelUse.features._modules['0'] = nn.Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "                modelUse.classifier._modules['6'] = nn.Linear(in_features=4096, out_features=numClasses, bias=True)\n",
    "            \n",
    "            \n",
    "            elif modelName == \"shufflenet_v2_x2_0\":\n",
    "                modelUse = models.shufflenet_v2_x2_0(pretrained=usePretrained)\n",
    "                setParameterRequiresGrad(modelUse, featureExtract)\n",
    "                if mono:\n",
    "                    #modelUse.conv1._modules['0'] = nn.Conv2d(1, 24, kernel_size=ksize, stride=ssize, padding=(1, 1), bias=False)\n",
    "                    modelUse.conv1._modules['0'] = nn.Conv2d(1, 24, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
    "                if dropout:\n",
    "                    modelUse.fc = nn.Sequential(nn.Dropout(dropNum),nn.Linear(2048, numClasses, bias=True))\n",
    "                                                \n",
    "                else:\n",
    "                    modelUse.fc = nn.Linear(2048, numClasses, bias=True)\n",
    "\n",
    "            elif modelName == \"shufflenet_v2_x0_5\":\n",
    "                modelUse = models.shufflenet_v2_x0_5(pretrained=usePretrained)\n",
    "                setParameterRequiresGrad(modelUse, featureExtract)\n",
    "                if mono:\n",
    "                    modelUse.conv1._modules['0'] = nn.Conv2d(1, 24, kernel_size=ksize, stride=ssize, padding=(1, 1), bias=False)\n",
    "\n",
    "                if dropout:\n",
    "                    modelUse.fc = nn.Sequential(nn.Dropout(dropNum),\n",
    "                                                nn.Linear(1024, numClasses, bias=True),\n",
    "                                                )\n",
    "                else:\n",
    "                    modelUse.fc = nn.Linear(1024, numClasses, bias=True)\n",
    "\n",
    "\n",
    "            elif modelName == \"shufflenet_v2_x1_5\":\n",
    "                modelUse = models.shufflenet_v2_x1_5(pretrained=usePretrained)\n",
    "                setParameterRequiresGrad(modelUse, featureExtract)\n",
    "                if mono:\n",
    "                    modelUse.conv1._modules['0'] = nn.Conv2d(1, 24, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
    "                    #modelUse.conv1._modules['0'] = nn.Conv2d(1, 24, kernel_size=ksize, stride=ssize, padding=(1, 1), bias=False)\n",
    "                if dropout:\n",
    "                    modelUse.fc = nn.Sequential(nn.Dropout(dropNum),\n",
    "                                                nn.Linear(1024, numClasses, bias=True),)\n",
    "                else:\n",
    "                    modelUse.fc = nn.Linear(1024, numClasses, bias=True)\n",
    "\n",
    "\n",
    "            elif modelName == \"shufflenet_v2_x1_0\":\n",
    "                modelUse = models.shufflenet_v2_x1_0(pretrained=usePretrained)\n",
    "                setParameterRequiresGrad(modelUse, featureExtract)\n",
    "                if mono:\n",
    "                    modelUse.conv1._modules['0'] = nn.Conv2d(1, 24, kernel_size=ksize, stride=ssize, padding=(1, 1), bias=False)\n",
    "\n",
    "                if dropout:\n",
    "                    modelUse.fc = nn.Sequential(nn.Dropout(dropNum),\n",
    "                                                nn.Linear(1024, numClasses, bias=True))\n",
    "                else:\n",
    "                    modelUse.fc = nn.Linear(1024, numClasses, bias=True)\n",
    "\n",
    "\n",
    "            elif modelName == \"squeezenet1_1\":\n",
    "                modelUse = models.squeezenet1_1(pretrained=usePretrained)\n",
    "                setParameterRequiresGrad(modelUse, featureExtract)\n",
    "                if mono:\n",
    "                    modelUse.features._modules['0'] = nn.Conv2d(1, 64, kernel_size=(3, 3), stride=(2, 2))\n",
    "\n",
    "                modelUse.classifier = nn.Sequential(nn.Dropout(p=dropNum, inplace=False),\n",
    "                                        nn.Conv2d(512, numClasses, kernel_size=(1, 1), stride=(1, 1)),\n",
    "                                        nn.ReLU(inplace=True),\n",
    "                                        nn.AdaptiveAvgPool2d(output_size=(1, 1)))\n",
    "\n",
    "\n",
    "\n",
    "            elif modelName == \"densenet121\":\n",
    "                modelUse = models.densenet121(pretrained=usePretrained)\n",
    "                setParameterRequiresGrad(modelUse, featureExtract)\n",
    "                if mono:\n",
    "                    modelUse.features._modules['conv0'] = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "\n",
    "                if dropout:\n",
    "                    modelUse.classifier = nn.Sequential(nn.Dropout(dropNum),\n",
    "                                                nn.Linear(1024, numClasses, bias=True))\n",
    "                else:\n",
    "                    modelUse.classifier = nn.Linear(1024, numClasses, bias=True)\n",
    "                    \n",
    "            elif modelName == \"densenet161\":\n",
    "                modelUse = models.densenet161(pretrained=usePretrained)\n",
    "                setParameterRequiresGrad(modelUse, featureExtract)\n",
    "                if mono:\n",
    "                    modelUse.features._modules['conv0'] = nn.Conv2d(1, 96, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "\n",
    "                if dropout:\n",
    "                    modelUse.classifier = nn.Sequential(nn.Dropout(dropNum),\n",
    "                                                nn.Linear(2208, numClasses, bias=True))\n",
    "                else:\n",
    "                    modelUse.classifier = nn.Linear(2208, numClasses, bias=True)\n",
    "\n",
    "\n",
    "            elif modelName == \"vgg16\":\n",
    "                modelUse = models.vgg16(pretrained=usePretrained)\n",
    "                setParameterRequiresGrad(modelUse, featureExtract)\n",
    "                if mono:\n",
    "                    modelUse.features._modules['0'] = nn.Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "                modelUse.classifier._modules['6'] = nn.Linear(in_features=4096, out_features=numClasses, bias=True)\n",
    "\n",
    "            elif modelName == \"mobilenet_v2\":\n",
    "                modelUse = models.mobilenet_v2(pretrained=usePretrained)\n",
    "                setParameterRequiresGrad(modelUse, featureExtract)\n",
    "                if mono:\n",
    "                    modelUse.features._modules['0']._modules['0'] = nn.Conv2d(1, 32, kernel_size=(3, 3), stride=(2, 2),\n",
    "                                                                              padding=(1, 1), bias=False)\n",
    "                    #modelUse.features._modules['0']._modules['0'] = nn.Conv2d(1, 32, kernel_size=ksize, stride=ssize,\n",
    "                    #                                                          padding=(1, 1), bias=False)\n",
    "                if dropout:\n",
    "                    modelUse.classifier = nn.Sequential(nn.Dropout(p=dropNum, inplace=False),\n",
    "                                                        nn.Linear(in_features=1280, out_features=numClasses, bias=True))\n",
    "\n",
    "            elif modelName == \"resnet18\":\n",
    "                modelUse = models.resnet18(pretrained=usePretrained)\n",
    "                setParameterRequiresGrad(modelUse, featureExtract)\n",
    "                numFeatures = modelUse.fc.in_features\n",
    "                if mono:\n",
    "                    modelUse.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "\n",
    "                if dropout:\n",
    "                    modelUse.fc = nn.Sequential(nn.Dropout(dropNum),\n",
    "                                                nn.Linear(numFeatures, numClasses))\n",
    "                else:\n",
    "                    modelUse.fc = nn.Linear(numFeatures, numClasses)\n",
    "\n",
    "            elif modelName == \"resnet34\":\n",
    "                modelUse = models.resnet34(pretrained=usePretrained)\n",
    "                setParameterRequiresGrad(modelUse, featureExtract)\n",
    "                numFeatures = modelUse.fc.in_features\n",
    "                if mono:\n",
    "                    modelUse.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "\n",
    "                if dropout:\n",
    "                    modelUse.fc = nn.Sequential(nn.Dropout(dropNum),\n",
    "                                                nn.Linear(numFeatures, numClasses))\n",
    "                else:\n",
    "                    modelUse.fc = nn.Linear(numFeatures, numClasses)\n",
    "\n",
    "            elif modelName == \"resnet50\":\n",
    "                modelUse = models.resnet50(pretrained=usePretrained)\n",
    "                setParameterRequiresGrad(modelUse, featureExtract)\n",
    "                numFeatures = modelUse.fc.in_features\n",
    "                if mono:\n",
    "                    modelUse.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "\n",
    "                if dropout:\n",
    "                    modelUse.fc = nn.Sequential(nn.Dropout(dropNum),\n",
    "                                                nn.Linear(numFeatures, numClasses))\n",
    "                else:\n",
    "                    modelUse.fc = nn.Linear(numFeatures, numClasses)\n",
    "                    \n",
    "            elif modelName == \"inception_v3\":\n",
    "                modelUse = models.inception_v3(pretrained=usePretrained,aux_logits=False)\n",
    "                setParameterRequiresGrad(modelUse, featureExtract)\n",
    "                if mono:\n",
    "                    modelUse.Conv2d_1a_3x3._modules['conv']=nn.Conv2d(1, 32, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
    "                modelUse.fc = nn.Linear(in_features=2048, out_features=numClasses, bias=True)      \n",
    "            \n",
    "            elif modelName == \"googlenet\":\n",
    "                modelUse = models.googlenet(pretrained=usePretrained)\n",
    "                setParameterRequiresGrad(modelUse, featureExtract)\n",
    "                if mono:\n",
    "                    modelUse.conv1._modules['conv']=nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "                modelUse.fc = nn.Linear(in_features=1024, out_features=numClasses, bias=True)   \n",
    "            \n",
    "\n",
    "            else:\n",
    "                print(\"model not implemented\")\n",
    "                return None, None\n",
    "\n",
    "            return modelUse\n",
    "\n",
    "\n",
    "        def trainModel(model, trainLoader, testLoader, lossFn, optimizer, numEpochs):\n",
    "            # if len(trainLoader) > len(testLoader):\n",
    "            #     phase = 'trai'\n",
    "            bestAccuracy = 0\n",
    "            bestModuleWeight = copy.deepcopy(model.state_dict())\n",
    "            trainAccuracyHistory = []\n",
    "            trainLossHistory = []\n",
    "            testAccuracyHistory = []\n",
    "            testLossHistory = []\n",
    "            for epoch in np.arange(numEpochs):\n",
    "                runningLoss = 0.\n",
    "                runningAccuracy = 0.\n",
    "                model.train()\n",
    "                for inputs, labels in trainLoader:\n",
    "                    inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                    with torch.autograd.set_grad_enabled(True):\n",
    "                        #outputs = model(inputs)  # bsize * 2\n",
    "                        #loss = lossFn(outputs, labels)\n",
    "    \n",
    "                        if modelName==\"googlenet\": \n",
    "                            aux1, aux2, outputs = model(inputs)     \n",
    "                            loss1 = lossFn(outputs, labels)\n",
    "                            loss2 = lossFn(aux1, labels)\n",
    "                            loss3 = lossFn(aux2, labels)\n",
    "                            loss = loss1 + 0.3*(loss2+loss3)\n",
    "\n",
    "                        else:\n",
    "                            outputs = model(inputs)  # bsize * 2\n",
    "                            loss = lossFn(outputs, labels)\n",
    "\n",
    "                    preds = outputs.argmax(dim=1)\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    runningLoss += loss.item() * inputs.size(0)\n",
    "                    runningAccuracy += torch.sum(preds.view(-1) == labels.view(-1)).item()\n",
    "                epochLoss = runningLoss / len(trainLoader.dataset)\n",
    "                epochAcc = runningAccuracy / len(trainLoader.dataset)\n",
    "                print(\"Epoch: {} Phase: {} loss: {}, acc: {}\".format(epoch, 'train', epochLoss, epochAcc))\n",
    "\n",
    "                trainLossHistory.append(epochLoss)\n",
    "                trainAccuracyHistory.append(epochAcc)\n",
    "\n",
    "                # bestAccuracy = 0\n",
    "                # bestModuleWeight = copy.deepcopy(model.state_dict())\n",
    "                # trainAccuracyHistory = []\n",
    "                # trainLossHistory = []\n",
    "                # testAccuracyHistory = []\n",
    "                # testLossHistory = []\n",
    "\n",
    "                runningLoss = 0.\n",
    "                runningAccuracy = 0.\n",
    "                model.eval()\n",
    "                if savePreAndRealFlag:\n",
    "                    preResult = []\n",
    "                    realResult = []\n",
    "\n",
    "                topList = []\n",
    "                for inputs, labels in testLoader:\n",
    "                    inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                    with torch.autograd.set_grad_enabled(False):\n",
    "                        outputs = model(inputs)  # bsize * 2\n",
    "                        loss = lossFn(outputs, labels)\n",
    "\n",
    "                    if epoch>numEpoch-5:\n",
    "                        softMax = nn.Softmax(dim=1)\n",
    "                        out = softMax(outputs)\n",
    "                        top = out.topk(1, dim=1)\n",
    "                        topScore = top[0].cpu().numpy()\n",
    "                        topList.append(topScore)\n",
    "                        topFile = open(filePath+'fileOutput/' + '7wb_topScore.txt', 'w')\n",
    "                        topsList = str(topList).replace(' ', '').replace(',dtype=float32)', '').replace(\"\\n\", '').replace(\n",
    "                            'array(', '').replace('[', '').replace('],', ' ').replace('],', ' ').replace(']', '').split(' ')\n",
    "                        for sortNum in np.arange(len(topsList)):\n",
    "                            # tops = topsList[sortNum]\n",
    "                            tops = \"%06d\" % sortNum + ': ' + str(topsList[sortNum]) + '\\n'\n",
    "                            topFile.write(tops)\n",
    "                        topFile.close()\n",
    "\n",
    "\n",
    "\n",
    "                    preds = outputs.argmax(dim=1)\n",
    "                    runningLoss += loss.item() * inputs.size(0)\n",
    "                    runningAccuracy += torch.sum(preds.view(-1) == labels.view(-1)).item()\n",
    "\n",
    "                    if epoch>numEpoch-5:\n",
    "                        if savePreAndRealFlag:\n",
    "                            preResult.append(str(preds.view(-1).cpu().numpy()))\n",
    "                            realResult.append(str(labels.view(-1).cpu().numpy()))\n",
    "                epochLoss = runningLoss / len(testLoader.dataset)\n",
    "                epochAcc = runningAccuracy / len(testLoader.dataset)\n",
    "                print(\"Epoch: {} Phase: {} loss: {}, acc: {}\".format(epoch, 'test', epochLoss, epochAcc))\n",
    "\n",
    "                if epochAcc > bestAccuracy:\n",
    "                    bestAccuracy = epochAcc\n",
    "                    bestModuleWeight = copy.deepcopy(model.state_dict())\n",
    "\n",
    "                testLossHistory.append(epochLoss)\n",
    "                testAccuracyHistory.append(epochAcc)\n",
    "               # if epoch % 5 == 0:\n",
    "                   # savePath2 = savePath + str(epoch) + '.pt'\n",
    "                   # torch.save(model.load_state_dict(bestModuleWeight), savePath2)\n",
    "            if epoch >numEpoch-5:\n",
    "                if savePreAndRealFlag:\n",
    "                    preFile = open(filePath +'fileOutput/' + modelName + time.strftime('%m%d%H%M')+ '_wb12_pre.txt', 'w')\n",
    "                    preFile.write(str(preResult))\n",
    "                    realFile = open(filePath+'fileOutput/' + modelName + time.strftime('%m%d%H%M')+ '_wb12_real.txt', 'w')\n",
    "                    realFile.write(str(realResult))\n",
    "                    preFile.close()\n",
    "                    realFile.close()\n",
    "            model.load_state_dict(bestModuleWeight)\n",
    "            return model, trainLossHistory, trainAccuracyHistory, testLossHistory, testAccuracyHistory\n",
    "\n",
    "\n",
    "        modelUse = initialModel(modelName, numClasses, featureExtract=False, usePretrained=False)\n",
    "        modelUse = modelUse.to(device)\n",
    "        optimizer = torch.optim.Adam(modelUse.parameters(), lr=learningRate)\n",
    "        lossFn = nn.CrossEntropyLoss()\n",
    "\n",
    "        modelReturn, trHis, trAcc, teHis, teAcc = trainModel(modelUse, trainDataloader, testDataloader, lossFn, optimizer,numEpoch)\n",
    "        torch.save(modelReturn, savePath + time.strftime('%m%d%H%M') + 'wb12_res01_batch16.pt')\n",
    "        torch.save(modelReturn.state_dict, savePath + time.strftime('%m%d%H%M') + '_Para_res01_batch16.pt')\n",
    "\n",
    "        trainaccFile = open(savePath+ time.strftime('%m%d%H%M')+ '_trainacc.txt', 'w')\n",
    "        trainaccFile.write(str(trAcc).replace(\"[\", \"\").replace(\"]\", \"\").replace(\",\", \"\")) \n",
    "        trainaccFile.close()\n",
    "        trainlossFile = open(savePath+ time.strftime('%m%d%H%M')+ '_trainloss.txt', 'w')\n",
    "        trainlossFile.write(str(trHis).replace(\"[\", \"\").replace(\"]\", \"\").replace(\",\", \"\")) \n",
    "        trainlossFile.close()\n",
    "        testlossFile = open(savePath+ time.strftime('%m%d%H%M')+ '_testloss.txt', 'w')\n",
    "        testlossFile.write(str(teHis).replace(\"[\", \"\").replace(\"]\", \"\").replace(\",\", \"\")) \n",
    "        testlossFile.close()\n",
    "        testaccFile = open(savePath+ time.strftime('%m%d%H%M')+ '_testacc.txt', 'w')\n",
    "        testaccFile.write(str(teAcc).replace(\"[\", \"\").replace(\"]\", \"\").replace(\",\", \"\")) \n",
    "        testaccFile.close()\n",
    "\n",
    "        saveparams4lisaFile = open(savePath+ time.strftime('%m%d%H%M%S')+ '_playingwithparams4lisa.txt', 'w')\n",
    "        saveparams4lisaFile.write(str(modelName) +\" LearningRate = \" + str(learningRate) +\" featureExtract = \" + str(featureExtract) +\" Epochnumber = \" + str(numEpoch) +\" batchsize = \" + str(batchSize) +\" traininglist = \"+str(txtFile))\n",
    "        saveparams4lisaFile.close()\n",
    "        \n",
    "        \n",
    "        winsound.Beep(400, 500)\n",
    "\n",
    "        torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "boxed-fields",
   "metadata": {},
   "outputs": [],
   "source": [
    "import winsound\n",
    "winsound.Beep(400, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "printable-visitor",
   "metadata": {},
   "outputs": [],
   "source": [
    "featureExtract=False\n",
    "str(featureExtract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "contemporary-uniform",
   "metadata": {},
   "outputs": [],
   "source": [
    " torch.cuda.empty_cache()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "naval-vanilla",
   "metadata": {},
   "outputs": [],
   "source": [
    "learningRate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "behind-glenn",
   "metadata": {},
   "outputs": [],
   "source": [
    "featureExtract = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "multiple-jackson",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelName = \"shufflenet_v2_x1_5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mathematical-delicious",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "modelnameslist = [\"shufflenet_v2_x1_5\", \"shufflenet_v2_x2_0\"]\n",
    "for modelName in modelnameslist:\n",
    "    for lrindex in (1,2,3,4):\n",
    "        if lrindex == 1:\n",
    "            learningRate = 0.001\n",
    "            featureExtract = True              \n",
    "            print(1)\n",
    "            print(modelName)\n",
    "        if lrindex == 2:\n",
    "            learningRate = 0.01\n",
    "            featureExtract = True\n",
    "            print(2)\n",
    "            print(modelName)\n",
    "        if lrindex == 3:\n",
    "            learningRate = 0.001\n",
    "            featureExtract = False\n",
    "            print(3)\n",
    "            print(modelName)\n",
    "        if lrindex == 4:\n",
    "            learningRate = 0.01\n",
    "            featureExtract = False\n",
    "            print(4)\n",
    "            print(modelName)\n",
    "        ksize = (3,3)\n",
    "        ssize = (2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "horizontal-elimination",
   "metadata": {},
   "outputs": [],
   "source": [
    "    print(modelName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "official-leone",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "saveparams4lisaFile = open(savePath+ time.strftime('%m%d%H%M%S')+ '_playingwithparams4lisa.txt', 'w')\n",
    "saveparams4lisaFile.write(str(modelName) +\" LearningRate = \" + str(learningRate) +\" featureExtract = \" + str(featureExtract) +\" Epochnumber = \" + str(numEpoch) +\"ksize = \" + str(ksize) +\"ssize = \" + str(ssize))\n",
    "saveparams4lisaFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "egyptian-elizabeth",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import time\n",
    "savePath = 'C:/automated_classification/holographic_plankton_classification-main/modelGen/' + modelName + '_'\n",
    "modelnameslist = [\"shufflenet_v2_x1_5\"]\n",
    "for modelName in modelnameslist:\n",
    "    for lrindex in (1,2,3,4):\n",
    "        if lrindex == 1:\n",
    "            learningRate = 0.00001\n",
    "            featureExtract = True\n",
    "            batchSize = 64\n",
    "            numEpoch = 500\n",
    "            ksize = (3,3)\n",
    "            ssize = (2,2)\n",
    "        if lrindex == 2:\n",
    "            learningRate = 0.00001\n",
    "            featureExtract = True\n",
    "            numEpoch = 500\n",
    "            batchSize = 64\n",
    "            ksize = (3,3)\n",
    "            ssize = (1,1)\n",
    "        if lrindex == 3:\n",
    "            learningRate = 0.00001\n",
    "            featureExtract = False\n",
    "            numEpoch = 700\n",
    "            batchSize = 32\n",
    "            ksize = (2,2)\n",
    "            ssize = (1,1)\n",
    "        if lrindex == 4:\n",
    "            learningRate = 0.001\n",
    "            featureExtract = False\n",
    "            numEpoch = 700\n",
    "            momentum = 0.5\n",
    "            batchSize = 64\n",
    "            ksize = (3,3)\n",
    "            ssize = (2,2)\n",
    "        saveparams4lisaFile = open(savePath+ time.strftime('%m%d%H%M%S')+ '_playingwithparams4lisa.txt', 'w')\n",
    "        saveparams4lisaFile.write(str(modelName) +\" LearningRate = \" + str(learningRate) +\" featureExtract = \" + str(featureExtract) +\" Epochnumber = \" + str(numEpoch) +\" batchsize = \" + str(batchSize)+\"ksize = \" + str(ksize) +\"ssize = \" + str(ssize))\n",
    "        saveparams4lisaFile.close()\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "thirty-blank",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'(2, 2)'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(ssize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "transsexual-freight",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "for lrindex in [1]:\n",
    "        if lrindex == 1:\n",
    "            learningRate = 0.0001\n",
    "            print(4)\n",
    "        if lrindex == 3:\n",
    "            learningRate = 0.0001\n",
    "            print(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "metallic-praise",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import microPlankton\n",
    "txtFile = \"new_set_train_v7.txt\"\n",
    "imagDataset = microPlankton(root=filePath, dataTXT=txtFile, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "familiar-distance",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "efficient-capacity",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "horizontal-hypothesis",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inception_v3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "print(modelName)\n",
    "if modelName==\"inception_v3\":\n",
    "    outputs, aux_outputs = model(inputs)\n",
    "    loss1 = lossFn(outputs, labels)\n",
    "    loss2 = lossFn(aux_outputs, labels)\n",
    "    loss = loss1 + 0.4*loss2\n",
    "    \n",
    "elif modelName==\"googlenet\": \n",
    "    aux1, aux2, outputs = model(inputs)     \n",
    "    loss1 = lossFn(outputs, labels)\n",
    "    loss2 = lossFn(aux1, labels)\n",
    "    loss3 = lossFn(aux2, labels)\n",
    "    loss = loss1 + 0.3*(loss2+loss3)\n",
    "\n",
    "else:\n",
    "    outputs = model(inputs)  # bsize * 2\n",
    "    loss = lossFn(outputs, labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "large-amsterdam",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coordinate-teacher",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blond-isaac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "462.997px",
    "left": "985px",
    "right": "20px",
    "top": "150.991px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
