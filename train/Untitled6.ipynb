{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bronze-lingerie",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "-2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-74e1214feb48>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[0mimagDataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmicroPlankton\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfilePath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataTXT\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtxtFile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 89\u001b[1;33m \u001b[0mtrain_set\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_set\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimagDataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtestSize\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m42\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     90\u001b[0m \u001b[0mtrainSet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataTran\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputSize\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdataTransforms\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"trainData\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[0mtestSet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataTran\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputSize\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_set\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdataTransforms\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"testData\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\automated_class\\lib\\site-packages\\sklearn\\model_selection\\_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[1;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[0;32m   2197\u001b[0m         \u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstratify\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2199\u001b[1;33m     return list(chain.from_iterable((_safe_indexing(a, train),\n\u001b[0m\u001b[0;32m   2200\u001b[0m                                      _safe_indexing(a, test)) for a in arrays))\n\u001b[0;32m   2201\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\automated_class\\lib\\site-packages\\sklearn\\model_selection\\_split.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   2197\u001b[0m         \u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstratify\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2199\u001b[1;33m     return list(chain.from_iterable((_safe_indexing(a, train),\n\u001b[0m\u001b[0;32m   2200\u001b[0m                                      _safe_indexing(a, test)) for a in arrays))\n\u001b[0;32m   2201\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\automated_class\\lib\\site-packages\\sklearn\\utils\\__init__.py\u001b[0m in \u001b[0;36m_safe_indexing\u001b[1;34m(X, indices, axis)\u001b[0m\n\u001b[0;32m    344\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_array_indexing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices_dtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    345\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 346\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_list_indexing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    347\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    348\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\automated_class\\lib\\site-packages\\sklearn\\utils\\__init__.py\u001b[0m in \u001b[0;36m_list_indexing\u001b[1;34m(X, key, key_dtype)\u001b[0m\n\u001b[0;32m    203\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcompress\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    204\u001b[0m     \u001b[1;31m# key is a integer array-like of key\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 205\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    206\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\automated_class\\lib\\site-packages\\sklearn\\utils\\__init__.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    203\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcompress\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    204\u001b[0m     \u001b[1;31m# key is a integer array-like of key\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 205\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    206\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\automated_classification\\holographic_plankton_classification-main\\train\\data.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, item)\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[0mimgDataPath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimgs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m         \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimgDataPath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m             \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\automated_classification\\holographic_plankton_classification-main\\train\\data.py\u001b[0m in \u001b[0;36mdefault_loader\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mdefault_loader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'F'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mmicroPlankton\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\automated_class\\lib\\site-packages\\PIL\\Image.py\u001b[0m in \u001b[0;36mconvert\u001b[1;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[0;32m    902\u001b[0m         \"\"\"\n\u001b[0;32m    903\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 904\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    905\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    906\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"P\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\automated_class\\lib\\site-packages\\PIL\\TiffImagePlugin.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1086\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1087\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtile\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muse_load_libtiff\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1088\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_load_libtiff\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1089\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1090\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\automated_class\\lib\\site-packages\\PIL\\TiffImagePlugin.py\u001b[0m in \u001b[0;36m_load_libtiff\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1191\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0merr\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1192\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mOSError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1193\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1194\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mImage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: -2"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms, models\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.autograd import Variable\n",
    "import torch.utils.data as Data\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from data import microPlankton\n",
    "import cv2 as cv\n",
    "from torchvision import transforms\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, train_test_split\n",
    "from dataTransformNew import dataTran\n",
    "import time\n",
    "\n",
    "batchSize = 10  # 11111111111\n",
    "numClasses = 1\n",
    "numEpoch = 1\n",
    "testSize = 0.5  # 11111111111\n",
    "learningRate = 0.01  # 11111111111\n",
    "dropNum = 0.5\n",
    "momentum = 0.9\n",
    "\n",
    "featureExtract = True\n",
    "usePretrained = False\n",
    "dropout = True\n",
    "mono = True\n",
    "savePreAndRealFlag = True\n",
    "\n",
    "samplePath = 'C:/automated_classification/holographic_plankton_classification-main/samples/'\n",
    "inputSize = 400\n",
    "imgType = '.tif'\n",
    "txtFile = \"a.txt\"  # 11111111111\n",
    "modelName = \"shufflenet_v2_x1_5\"\n",
    "filePath = \"C:/automated_classification/holographic_plankton_classification-main/\"\n",
    "savePath = 'C:/automated_classification/holographic_plankton_classification-main/modelGen/' + modelName + '_'\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "# --------------------find the input size for padding START\n",
    "def findMax(imgPath, fileType):\n",
    "    maxPixel = 0\n",
    "    for mainPath, dirs, file in os.walk(imgPath, topdown=False):\n",
    "        for subFolderName in dirs:  # read subfolder\n",
    "            pathNow = os.path.join(mainPath, subFolderName)\n",
    "            for subMainPath, subDirs, subFile in os.walk(pathNow, topdown=False):  # find data file\n",
    "                for dataFile in subFile:  # read data file\n",
    "                    if os.path.splitext(dataFile)[1] == fileType:  # find data with specific suffix\n",
    "                        img = cv.imread(subMainPath + '/' + dataFile)\n",
    "                        try:\n",
    "                            tempPixel = max(img.shape)\n",
    "                        except:\n",
    "                            print(dataFile)\n",
    "                        maxPixel = max(tempPixel, maxPixel)\n",
    "    return maxPixel\n",
    "\n",
    "\n",
    "# inputSize = findMax(samplePath, imgType)\n",
    "# --------------------find the input size for padding END\n",
    "\n",
    "dataTransforms = {\n",
    "    \"trainData\": transforms.Compose([\n",
    "        # transforms.Resize(inputSize),\n",
    "        # transforms.RandomCrop(size=inputSize, pad_if_needed=True, padding_mode='constant'),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),  # horizontal flip and 0.5 is the position\n",
    "        transforms.RandomVerticalFlip(p=0.5),\n",
    "        torchvision.transforms.RandomRotation(45, resample=False, expand=False,\n",
    "                                              center=None),\n",
    "        transforms.ToTensor(),\n",
    "        # transforms.Normalize([],[])\n",
    "    ]),\n",
    "    \"testData\": transforms.Compose([\n",
    "        # transforms.Resize(inputSize),\n",
    "        # transforms.RandomCrop(size=inputSize, pad_if_needed=True, padding_mode='constant'),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),  # horizontal flip and 0.5 is the position\n",
    "        transforms.RandomVerticalFlip(p=0.5),\n",
    "        transforms.ToTensor(),\n",
    "        # transforms.Normalize([],[])\n",
    "    ])\n",
    "}\n",
    "\n",
    "imagDataset = microPlankton(root=filePath, dataTXT=txtFile, )\n",
    "train_set, test_set = train_test_split(imagDataset, test_size=testSize,random_state=42)\n",
    "trainSet = dataTran(inputSize,train_set, transform=dataTransforms[\"trainData\"])\n",
    "testSet = dataTran(inputSize,test_set, transform=dataTransforms[\"testData\"])\n",
    "testDataloader = torch.utils.data.DataLoader(testSet, batch_size=batchSize, shuffle=True, num_workers=24,\n",
    "                                             pin_memory=True)\n",
    "trainDataloader = torch.utils.data.DataLoader(trainSet, batch_size=batchSize, shuffle=True, num_workers=24,\n",
    "                                              pin_memory=True)\n",
    "\n",
    "\n",
    "# -------------copy from resnet\n",
    "\n",
    "\n",
    "# --------------------Show imgs in the dataset Start\n",
    "# imgs = next(iter(trainDataloader['trainData']))[0]\n",
    "# unloader = transforms.ToPILImage()\n",
    "#\n",
    "# def imgShow( tensor, title):\n",
    "#     img = tensor.cpu().clone()\n",
    "#     img = img.squeeze(0)\n",
    "#     img = unloader(img)\n",
    "#     plt.imshow(img, cmap=\"gray\")\n",
    "#     if title is not None:\n",
    "#         plt.title(title)\n",
    "#     plt.pause(0.001)\n",
    "#\n",
    "# # print(imgs.shape)\n",
    "# plt.figure()\n",
    "# imgShow(imgs[3],title=\"img\")\n",
    "\n",
    "# --------------------Show imgs in the dataset End\n",
    "\n",
    "\n",
    "def setParameterRequiresGrad(model, featureExtract):\n",
    "    if featureExtract:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "\n",
    "def initialModel(modelName, numClasses, featureExtract, usePretrained):\n",
    "    if modelName == \"vgg19\":\n",
    "        modelUse = models.vgg19(pretrained=usePretrained)\n",
    "        setParameterRequiresGrad(modelUse, featureExtract)\n",
    "        if mono:\n",
    "            modelUse.features._modules['0'] = nn.Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "        modelUse.classifier._modules['6'] = nn.Linear(in_features=4096, out_features=numClasses, bias=True)\n",
    "\n",
    "    elif modelName == \"shufflenet_v2_x2_0\":\n",
    "        modelUse = models.shufflenet_v2_x2_0(pretrained=usePretrained)\n",
    "        setParameterRequiresGrad(modelUse, featureExtract)\n",
    "        if mono:\n",
    "            modelUse.conv1._modules['0'] = nn.Conv2d(1, 24, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
    "\n",
    "        if dropout:\n",
    "            modelUse.fc = nn.Sequential(nn.Dropout(dropNum),\n",
    "                                        nn.Linear(2048, numClasses, bias=True))\n",
    "        else:\n",
    "            modelUse.fc = nn.Linear(2048, numClasses, bias=True)\n",
    "\n",
    "    elif modelName == \"shufflenet_v2_x0_5\":\n",
    "        modelUse = models.shufflenet_v2_x0_5(pretrained=usePretrained)\n",
    "        setParameterRequiresGrad(modelUse, featureExtract)\n",
    "        if mono:\n",
    "            modelUse.conv1._modules['0'] = nn.Conv2d(1, 24, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
    "\n",
    "        if dropout:\n",
    "            modelUse.fc = nn.Sequential(nn.Dropout(dropNum),\n",
    "                                        nn.Linear(1024, numClasses, bias=True),\n",
    "                                        )\n",
    "        else:\n",
    "            modelUse.fc = nn.Linear(1024, numClasses, bias=True)\n",
    "\n",
    "\n",
    "    elif modelName == \"shufflenet_v2_x1_5\":\n",
    "        modelUse = models.shufflenet_v2_x1_5(pretrained=usePretrained)\n",
    "        setParameterRequiresGrad(modelUse, featureExtract)\n",
    "        if mono:\n",
    "            modelUse.conv1._modules['0'] = nn.Conv2d(1, 24, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
    "\n",
    "        if dropout:\n",
    "            modelUse.fc = nn.Sequential(nn.Dropout(dropNum),\n",
    "                                        nn.Linear(1024, numClasses, bias=True),)\n",
    "        else:\n",
    "            modelUse.fc = nn.Linear(1024, numClasses, bias=True)\n",
    "\n",
    "\n",
    "    elif modelName == \"shufflenet_v2_x1_0\":\n",
    "        modelUse = models.shufflenet_v2_x1_0(pretrained=usePretrained)\n",
    "        setParameterRequiresGrad(modelUse, featureExtract)\n",
    "        if mono:\n",
    "            modelUse.conv1._modules['0'] = nn.Conv2d(1, 24, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
    "\n",
    "        if dropout:\n",
    "            modelUse.fc = nn.Sequential(nn.Dropout(dropNum),\n",
    "                                        nn.Linear(1024, numClasses, bias=True))\n",
    "        else:\n",
    "            modelUse.fc = nn.Linear(1024, numClasses, bias=True)\n",
    "\n",
    "\n",
    "    elif modelName == \"squeezenet1_1\":\n",
    "        modelUse = models.squeezenet1_1(pretrained=usePretrained)\n",
    "        setParameterRequiresGrad(modelUse, featureExtract)\n",
    "        if mono:\n",
    "            modelUse.features._modules['0'] = nn.Conv2d(1, 64, kernel_size=(3, 3), stride=(2, 2))\n",
    "\n",
    "        modelUse.classifier = nn.Sequential(nn.Dropout(p=dropNum, inplace=False),\n",
    "                                nn.Conv2d(512, numClasses, kernel_size=(1, 1), stride=(1, 1)),\n",
    "                                nn.ReLU(inplace=True),\n",
    "                                nn.AdaptiveAvgPool2d(output_size=(1, 1)))\n",
    "\n",
    "\n",
    "\n",
    "    elif modelName == \"densenet121\":\n",
    "        modelUse = models.densenet121(pretrained=usePretrained)\n",
    "        setParameterRequiresGrad(modelUse, featureExtract)\n",
    "        if mono:\n",
    "            modelUse.features._modules['conv0'] = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "\n",
    "        if dropout:\n",
    "            modelUse.classifier = nn.Sequential(nn.Dropout(dropNum),\n",
    "                                        nn.Linear(1024, numClasses, bias=True))\n",
    "        else:\n",
    "            modelUse.classifier = nn.Linear(1024, numClasses, bias=True)\n",
    "\n",
    "\n",
    "    elif modelName == \"vgg16\":\n",
    "        modelUse = models.vgg16(pretrained=usePretrained)\n",
    "        setParameterRequiresGrad(modelUse, featureExtract)\n",
    "        if mono:\n",
    "            modelUse.features._modules['0'] = nn.Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "        modelUse.classifier._modules['6'] = nn.Linear(in_features=4096, out_features=numClasses, bias=True)\n",
    "\n",
    "    elif modelName == 'mobilenet_v2':\n",
    "        modelUse = models.mobilenet_v2(pretrained=usePretrained)\n",
    "        setParameterRequiresGrad(modelUse, featureExtract)\n",
    "        if mono:\n",
    "            modelUse.features._modules['0']._modules['0'] = nn.Conv2d(1, 32, kernel_size=(3, 3), stride=(2, 2),\n",
    "                                                                      padding=(1, 1), bias=False)\n",
    "\n",
    "        modelUse.classifier = nn.Sequential(\n",
    "            nn.Dropout(p=dropNum, inplace=False),\n",
    "            nn.Linear(in_features=1280, out_features=numClasses, bias=True)\n",
    "        )\n",
    "\n",
    "    elif modelName == \"resnet18\":\n",
    "        modelUse = models.resnet18(pretrained=usePretrained)\n",
    "        setParameterRequiresGrad(modelUse, featureExtract)\n",
    "        numFeatures = modelUse.fc.in_features\n",
    "        if mono:\n",
    "            modelUse.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "\n",
    "        if dropout:\n",
    "            modelUse.fc = nn.Sequential(nn.Dropout(dropNum),\n",
    "                                        nn.Linear(numFeatures, numClasses))\n",
    "        else:\n",
    "            modelUse.fc = nn.Linear(numFeatures, numClasses)\n",
    "\n",
    "    elif modelName == \"resnet34\":\n",
    "        modelUse = models.resnet34(pretrained=usePretrained)\n",
    "        setParameterRequiresGrad(modelUse, featureExtract)\n",
    "        numFeatures = modelUse.fc.in_features\n",
    "        if mono:\n",
    "            modelUse.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "\n",
    "        if dropout:\n",
    "            modelUse.fc = nn.Sequential(nn.Dropout(dropNum),\n",
    "                                        nn.Linear(numFeatures, numClasses))\n",
    "        else:\n",
    "            modelUse.fc = nn.Linear(numFeatures, numClasses)\n",
    "\n",
    "    elif modelName == \"resnet50\":\n",
    "        modelUse = models.resnet50(pretrained=usePretrained)\n",
    "        setParameterRequiresGrad(modelUse, featureExtract)\n",
    "        numFeatures = modelUse.fc.in_features\n",
    "        if mono:\n",
    "            modelUse.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "\n",
    "        if dropout:\n",
    "            modelUse.fc = nn.Sequential(nn.Dropout(dropNum),\n",
    "                                        nn.Linear(numFeatures, numClasses))\n",
    "        else:\n",
    "            modelUse.fc = nn.Linear(numFeatures, numClasses)\n",
    "\n",
    "    else:\n",
    "        print(\"model not implemented\")\n",
    "        return None, None\n",
    "\n",
    "    return modelUse\n",
    "\n",
    "\n",
    "def trainModel(model, trainLoader, testLoader, lossFn, optimizer, numEpochs):\n",
    "    # if len(trainLoader) > len(testLoader):\n",
    "    #     phase = 'trai'\n",
    "    bestAccuracy = 0\n",
    "    bestModuleWeight = copy.deepcopy(model.state_dict())\n",
    "    trainAccuracyHistory = []\n",
    "    trainLossHistory = []\n",
    "    testAccuracyHistory = []\n",
    "    testLossHistory = []\n",
    "    for epoch in np.arange(numEpochs):\n",
    "        runningLoss = 0.\n",
    "        runningAccuracy = 0.\n",
    "        model.train()\n",
    "        for inputs, labels in trainLoader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            with torch.autograd.set_grad_enabled(True):\n",
    "                outputs = model(inputs)  # bsize * 2\n",
    "                loss = lossFn(outputs, labels)\n",
    "\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            runningLoss += loss.item() * inputs.size(0)\n",
    "            runningAccuracy += torch.sum(preds.view(-1) == labels.view(-1)).item()\n",
    "        epochLoss = runningLoss / len(trainLoader.dataset)\n",
    "        epochAcc = runningAccuracy / len(trainLoader.dataset)\n",
    "        print(\"Epoch: {} Phase: {} loss: {}, acc: {}\".format(epoch, 'train', epochLoss, epochAcc))\n",
    "\n",
    "        trainLossHistory.append(epochLoss)\n",
    "        trainAccuracyHistory.append(epochAcc)\n",
    "\n",
    "        # bestAccuracy = 0\n",
    "        # bestModuleWeight = copy.deepcopy(model.state_dict())\n",
    "        # trainAccuracyHistory = []\n",
    "        # trainLossHistory = []\n",
    "        # testAccuracyHistory = []\n",
    "        # testLossHistory = []\n",
    "\n",
    "        runningLoss = 0.\n",
    "        runningAccuracy = 0.\n",
    "        model.eval()\n",
    "        if savePreAndRealFlag:\n",
    "            preResult = []\n",
    "            realResult = []\n",
    "\n",
    "        topList = []\n",
    "        for inputs, labels in testLoader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            with torch.autograd.set_grad_enabled(False):\n",
    "                outputs = model(inputs)  # bsize * 2\n",
    "                loss = lossFn(outputs, labels)\n",
    "\n",
    "            if epoch>295:\n",
    "                softMax = nn.Softmax(dim=1)\n",
    "                out = softMax(outputs)\n",
    "                top = out.topk(1, dim=1)\n",
    "                topScore = top[0].cpu().numpy()\n",
    "                topList.append(topScore)\n",
    "                topFile = open(filePath+'fileOutput/' + '7wb_topScore.txt', 'w')\n",
    "                topsList = str(topList).replace(' ', '').replace(',dtype=float32)', '').replace(\"\\n\", '').replace(\n",
    "                    'array(', '').replace('[', '').replace('],', ' ').replace('],', ' ').replace(']', '').split(' ')\n",
    "                for sortNum in np.arange(len(topsList)):\n",
    "                    # tops = topsList[sortNum]\n",
    "                    tops = \"%06d\" % sortNum + ': ' + str(topsList[sortNum]) + '\\n'\n",
    "                    topFile.write(tops)\n",
    "                topFile.close()\n",
    "\n",
    "\n",
    "\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            runningLoss += loss.item() * inputs.size(0)\n",
    "            runningAccuracy += torch.sum(preds.view(-1) == labels.view(-1)).item()\n",
    "\n",
    "            if epoch>295:\n",
    "                if savePreAndRealFlag:\n",
    "                    preResult.append(str(preds.view(-1).cpu().numpy()))\n",
    "                    realResult.append(str(labels.view(-1).cpu().numpy()))\n",
    "        epochLoss = runningLoss / len(testLoader.dataset)\n",
    "        epochAcc = runningAccuracy / len(testLoader.dataset)\n",
    "        print(\"Epoch: {} Phase: {} loss: {}, acc: {}\".format(epoch, 'test', epochLoss, epochAcc))\n",
    "\n",
    "        if epochAcc > bestAccuracy:\n",
    "            bestAccuracy = epochAcc\n",
    "            bestModuleWeight = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        testLossHistory.append(epochLoss)\n",
    "        testAccuracyHistory.append(epochAcc)\n",
    "        if epoch % 5 == 0:\n",
    "            savePath2 = savePath + str(epoch) + '.pt'\n",
    "            torch.save(model.load_state_dict(bestModuleWeight), savePath2)\n",
    "    if epoch >295:\n",
    "        if savePreAndRealFlag:\n",
    "            preFile = open(filePath +'fileOutput/' + modelName + time.strftime('%m%d%H%M')+ '_wb12_pre.txt', 'w')\n",
    "            preFile.write(str(preResult))\n",
    "            realFile = open(filePath+'fileOutput/' + modelName + time.strftime('%m%d%H%M')+ '_wb12_real.txt', 'w')\n",
    "            realFile.write(str(realResult))\n",
    "            preFile.close()\n",
    "            realFile.close()\n",
    "    model.load_state_dict(bestModuleWeight)\n",
    "    return model, trainLossHistory, trainAccuracyHistory, testLossHistory, testAccuracyHistory\n",
    "\n",
    "\n",
    "modelUse = initialModel(modelName, numClasses, featureExtract=False, usePretrained=False)\n",
    "modelUse = modelUse.to(device)\n",
    "optimizer = torch.optim.Adam(modelUse.parameters(), lr=learningRate)\n",
    "lossFn = nn.CrossEntropyLoss()\n",
    "\n",
    "modelReturn, trHis, trAcc, teHis, teAcc = trainModel(modelUse, trainDataloader, testDataloader, lossFn, optimizer,\n",
    "                                                     numEpoch)\n",
    "torch.save(modelReturn, savePath + time.strftime('%m%d%H%M') + 'wb12_res01_batch16.pt')\n",
    "torch.save(modelReturn.state_dict, savePath + time.strftime('%m%d%H%M') + '_Para_res01_batch16.pt')\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exciting-architecture",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
