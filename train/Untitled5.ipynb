{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "southeast-cemetery",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms, models\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.autograd import Variable\n",
    "import torch.utils.data as Data\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from data import microPlankton\n",
    "import cv2 as cv\n",
    "from torchvision import transforms\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, train_test_split\n",
    "from dataTransformNew import dataTran\n",
    "import time\n",
    "\n",
    "batchSize = 10  # 11111111111\n",
    "numClasses = 1\n",
    "numEpoch = 1\n",
    "testSize = 0.23  # 11111111111\n",
    "learningRate = 0.01  # 11111111111\n",
    "dropNum = 0.5\n",
    "momentum = 0.9\n",
    "\n",
    "featureExtract = True\n",
    "usePretrained = False\n",
    "dropout = True\n",
    "mono = True\n",
    "savePreAndRealFlag = True\n",
    "\n",
    "samplePath = 'C:/automated_classification/holographic_plankton_classification-main/samples/'\n",
    "inputSize = 400\n",
    "imgType = '.tif'\n",
    "txtFile = \"a.txt\"  # 11111111111\n",
    "modelName = \"shufflenet_v2_x1_5\"\n",
    "filePath = \"C:/automated_classification/holographic_plankton_classification-main/\"\n",
    "savePath = 'C:/automated_classification/holographic_plankton_classification-main/modelGen/' + modelName + '_'\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "# --------------------find the input size for padding START\n",
    "def findMax(imgPath, fileType):\n",
    "    maxPixel = 0\n",
    "    for mainPath, dirs, file in os.walk(imgPath, topdown=False):\n",
    "        for subFolderName in dirs:  # read subfolder\n",
    "            pathNow = os.path.join(mainPath, subFolderName)\n",
    "            for subMainPath, subDirs, subFile in os.walk(pathNow, topdown=False):  # find data file\n",
    "                for dataFile in subFile:  # read data file\n",
    "                    if os.path.splitext(dataFile)[1] == fileType:  # find data with specific suffix\n",
    "                        img = cv.imread(subMainPath + '/' + dataFile)\n",
    "                        try:\n",
    "                            tempPixel = max(img.shape)\n",
    "                        except:\n",
    "                            print(dataFile)\n",
    "                        maxPixel = max(tempPixel, maxPixel)\n",
    "    return maxPixel\n",
    "\n",
    "\n",
    "# inputSize = findMax(samplePath, imgType)\n",
    "# --------------------find the input size for padding END\n",
    "\n",
    "dataTransforms = {\n",
    "    \"trainData\": transforms.Compose([\n",
    "        # transforms.Resize(inputSize),\n",
    "        # transforms.RandomCrop(size=inputSize, pad_if_needed=True, padding_mode='constant'),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),  # horizontal flip and 0.5 is the position\n",
    "        transforms.RandomVerticalFlip(p=0.5),\n",
    "        torchvision.transforms.RandomRotation(45, resample=False, expand=False,\n",
    "                                              center=None),\n",
    "        transforms.ToTensor(),\n",
    "        # transforms.Normalize([],[])\n",
    "    ]),\n",
    "    \"testData\": transforms.Compose([\n",
    "        # transforms.Resize(inputSize),\n",
    "        # transforms.RandomCrop(size=inputSize, pad_if_needed=True, padding_mode='constant'),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),  # horizontal flip and 0.5 is the position\n",
    "        transforms.RandomVerticalFlip(p=0.5),\n",
    "        transforms.ToTensor(),\n",
    "        # transforms.Normalize([],[])\n",
    "    ])\n",
    "}\n",
    "\n",
    "imagDataset = microPlankton(root=filePath, dataTXT=txtFile, )\n",
    "train_set, test_set = train_test_split(imagDataset, test_size=testSize, random_state=1)\n",
    "trainSet = dataTran(inputSize,train_set, transform=dataTransforms[\"trainData\"])\n",
    "testSet = dataTran(inputSize,test_set, transform=dataTransforms[\"testData\"])\n",
    "testDataloader = torch.utils.data.DataLoader(testSet, batch_size=batchSize, shuffle=True, num_workers=24,\n",
    "                                             pin_memory=True)\n",
    "trainDataloader = torch.utils.data.DataLoader(trainSet, batch_size=batchSize, shuffle=True, num_workers=24,\n",
    "                                              pin_memory=True)\n",
    "\n",
    "\n",
    "# -------------copy from resnet\n",
    "\n",
    "\n",
    "# --------------------Show imgs in the dataset Start\n",
    "# imgs = next(iter(trainDataloader['trainData']))[0]\n",
    "# unloader = transforms.ToPILImage()\n",
    "#\n",
    "# def imgShow( tensor, title):\n",
    "#     img = tensor.cpu().clone()\n",
    "#     img = img.squeeze(0)\n",
    "#     img = unloader(img)\n",
    "#     plt.imshow(img, cmap=\"gray\")\n",
    "#     if title is not None:\n",
    "#         plt.title(title)\n",
    "#     plt.pause(0.001)\n",
    "#\n",
    "# # print(imgs.shape)\n",
    "# plt.figure()\n",
    "# imgShow(imgs[3],title=\"img\")\n",
    "\n",
    "# --------------------Show imgs in the dataset End\n",
    "\n",
    "\n",
    "def setParameterRequiresGrad(model, featureExtract):\n",
    "    if featureExtract:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "\n",
    "def initialModel(modelName, numClasses, featureExtract, usePretrained):\n",
    "    if modelName == \"vgg19\":\n",
    "        modelUse = models.vgg19(pretrained=usePretrained)\n",
    "        setParameterRequiresGrad(modelUse, featureExtract)\n",
    "        if mono:\n",
    "            modelUse.features._modules['0'] = nn.Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "        modelUse.classifier._modules['6'] = nn.Linear(in_features=4096, out_features=numClasses, bias=True)\n",
    "\n",
    "    elif modelName == \"shufflenet_v2_x2_0\":\n",
    "        modelUse = models.shufflenet_v2_x2_0(pretrained=usePretrained)\n",
    "        setParameterRequiresGrad(modelUse, featureExtract)\n",
    "        if mono:\n",
    "            modelUse.conv1._modules['0'] = nn.Conv2d(1, 24, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
    "\n",
    "        if dropout:\n",
    "            modelUse.fc = nn.Sequential(nn.Dropout(dropNum),\n",
    "                                        nn.Linear(2048, numClasses, bias=True))\n",
    "        else:\n",
    "            modelUse.fc = nn.Linear(2048, numClasses, bias=True)\n",
    "\n",
    "    elif modelName == \"shufflenet_v2_x0_5\":\n",
    "        modelUse = models.shufflenet_v2_x0_5(pretrained=usePretrained)\n",
    "        setParameterRequiresGrad(modelUse, featureExtract)\n",
    "        if mono:\n",
    "            modelUse.conv1._modules['0'] = nn.Conv2d(1, 24, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
    "\n",
    "        if dropout:\n",
    "            modelUse.fc = nn.Sequential(nn.Dropout(dropNum),\n",
    "                                        nn.Linear(1024, numClasses, bias=True),\n",
    "                                        )\n",
    "        else:\n",
    "            modelUse.fc = nn.Linear(1024, numClasses, bias=True)\n",
    "\n",
    "\n",
    "    elif modelName == \"shufflenet_v2_x1_5\":\n",
    "        modelUse = models.shufflenet_v2_x1_5(pretrained=usePretrained)\n",
    "        setParameterRequiresGrad(modelUse, featureExtract)\n",
    "        if mono:\n",
    "            modelUse.conv1._modules['0'] = nn.Conv2d(1, 24, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
    "\n",
    "        if dropout:\n",
    "            modelUse.fc = nn.Sequential(nn.Dropout(dropNum),\n",
    "                                        nn.Linear(1024, numClasses, bias=True),)\n",
    "        else:\n",
    "            modelUse.fc = nn.Linear(1024, numClasses, bias=True)\n",
    "\n",
    "\n",
    "    elif modelName == \"shufflenet_v2_x1_0\":\n",
    "        modelUse = models.shufflenet_v2_x1_0(pretrained=usePretrained)\n",
    "        setParameterRequiresGrad(modelUse, featureExtract)\n",
    "        if mono:\n",
    "            modelUse.conv1._modules['0'] = nn.Conv2d(1, 24, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
    "\n",
    "        if dropout:\n",
    "            modelUse.fc = nn.Sequential(nn.Dropout(dropNum),\n",
    "                                        nn.Linear(1024, numClasses, bias=True))\n",
    "        else:\n",
    "            modelUse.fc = nn.Linear(1024, numClasses, bias=True)\n",
    "\n",
    "\n",
    "    elif modelName == \"squeezenet1_1\":\n",
    "        modelUse = models.squeezenet1_1(pretrained=usePretrained)\n",
    "        setParameterRequiresGrad(modelUse, featureExtract)\n",
    "        if mono:\n",
    "            modelUse.features._modules['0'] = nn.Conv2d(1, 64, kernel_size=(3, 3), stride=(2, 2))\n",
    "\n",
    "        modelUse.classifier = nn.Sequential(nn.Dropout(p=dropNum, inplace=False),\n",
    "                                nn.Conv2d(512, numClasses, kernel_size=(1, 1), stride=(1, 1)),\n",
    "                                nn.ReLU(inplace=True),\n",
    "                                nn.AdaptiveAvgPool2d(output_size=(1, 1)))\n",
    "\n",
    "\n",
    "\n",
    "    elif modelName == \"densenet121\":\n",
    "        modelUse = models.densenet121(pretrained=usePretrained)\n",
    "        setParameterRequiresGrad(modelUse, featureExtract)\n",
    "        if mono:\n",
    "            modelUse.features._modules['conv0'] = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "\n",
    "        if dropout:\n",
    "            modelUse.classifier = nn.Sequential(nn.Dropout(dropNum),\n",
    "                                        nn.Linear(1024, numClasses, bias=True))\n",
    "        else:\n",
    "            modelUse.classifier = nn.Linear(1024, numClasses, bias=True)\n",
    "\n",
    "\n",
    "    elif modelName == \"vgg16\":\n",
    "        modelUse = models.vgg16(pretrained=usePretrained)\n",
    "        setParameterRequiresGrad(modelUse, featureExtract)\n",
    "        if mono:\n",
    "            modelUse.features._modules['0'] = nn.Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "        modelUse.classifier._modules['6'] = nn.Linear(in_features=4096, out_features=numClasses, bias=True)\n",
    "\n",
    "    elif modelName == 'mobilenet_v2':\n",
    "        modelUse = models.mobilenet_v2(pretrained=usePretrained)\n",
    "        setParameterRequiresGrad(modelUse, featureExtract)\n",
    "        if mono:\n",
    "            modelUse.features._modules['0']._modules['0'] = nn.Conv2d(1, 32, kernel_size=(3, 3), stride=(2, 2),\n",
    "                                                                      padding=(1, 1), bias=False)\n",
    "\n",
    "        modelUse.classifier = nn.Sequential(\n",
    "            nn.Dropout(p=dropNum, inplace=False),\n",
    "            nn.Linear(in_features=1280, out_features=numClasses, bias=True)\n",
    "        )\n",
    "\n",
    "    elif modelName == \"resnet18\":\n",
    "        modelUse = models.resnet18(pretrained=usePretrained)\n",
    "        setParameterRequiresGrad(modelUse, featureExtract)\n",
    "        numFeatures = modelUse.fc.in_features\n",
    "        if mono:\n",
    "            modelUse.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "\n",
    "        if dropout:\n",
    "            modelUse.fc = nn.Sequential(nn.Dropout(dropNum),\n",
    "                                        nn.Linear(numFeatures, numClasses))\n",
    "        else:\n",
    "            modelUse.fc = nn.Linear(numFeatures, numClasses)\n",
    "\n",
    "    elif modelName == \"resnet34\":\n",
    "        modelUse = models.resnet34(pretrained=usePretrained)\n",
    "        setParameterRequiresGrad(modelUse, featureExtract)\n",
    "        numFeatures = modelUse.fc.in_features\n",
    "        if mono:\n",
    "            modelUse.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "\n",
    "        if dropout:\n",
    "            modelUse.fc = nn.Sequential(nn.Dropout(dropNum),\n",
    "                                        nn.Linear(numFeatures, numClasses))\n",
    "        else:\n",
    "            modelUse.fc = nn.Linear(numFeatures, numClasses)\n",
    "\n",
    "    elif modelName == \"resnet50\":\n",
    "        modelUse = models.resnet50(pretrained=usePretrained)\n",
    "        setParameterRequiresGrad(modelUse, featureExtract)\n",
    "        numFeatures = modelUse.fc.in_features\n",
    "        if mono:\n",
    "            modelUse.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "\n",
    "        if dropout:\n",
    "            modelUse.fc = nn.Sequential(nn.Dropout(dropNum),\n",
    "                                        nn.Linear(numFeatures, numClasses))\n",
    "        else:\n",
    "            modelUse.fc = nn.Linear(numFeatures, numClasses)\n",
    "\n",
    "    else:\n",
    "        print(\"model not implemented\")\n",
    "        return None, None\n",
    "\n",
    "    return modelUse\n",
    "\n",
    "\n",
    "def trainModel(model, trainLoader, testLoader, lossFn, optimizer, numEpochs):\n",
    "    # if len(trainLoader) > len(testLoader):\n",
    "    #     phase = 'trai'\n",
    "    bestAccuracy = 0\n",
    "    bestModuleWeight = copy.deepcopy(model.state_dict())\n",
    "    trainAccuracyHistory = []\n",
    "    trainLossHistory = []\n",
    "    testAccuracyHistory = []\n",
    "    testLossHistory = []\n",
    "    for epoch in np.arange(numEpochs):\n",
    "        runningLoss = 0.\n",
    "        runningAccuracy = 0.\n",
    "        model.train()\n",
    "        for inputs, labels in trainLoader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            with torch.autograd.set_grad_enabled(True):\n",
    "                outputs = model(inputs)  # bsize * 2\n",
    "                loss = lossFn(outputs, labels)\n",
    "\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            runningLoss += loss.item() * inputs.size(0)\n",
    "            runningAccuracy += torch.sum(preds.view(-1) == labels.view(-1)).item()\n",
    "        epochLoss = runningLoss / len(trainLoader.dataset)\n",
    "        epochAcc = runningAccuracy / len(trainLoader.dataset)\n",
    "        print(\"Epoch: {} Phase: {} loss: {}, acc: {}\".format(epoch, 'train', epochLoss, epochAcc))\n",
    "\n",
    "        trainLossHistory.append(epochLoss)\n",
    "        trainAccuracyHistory.append(epochAcc)\n",
    "\n",
    "        # bestAccuracy = 0\n",
    "        # bestModuleWeight = copy.deepcopy(model.state_dict())\n",
    "        # trainAccuracyHistory = []\n",
    "        # trainLossHistory = []\n",
    "        # testAccuracyHistory = []\n",
    "        # testLossHistory = []\n",
    "\n",
    "        runningLoss = 0.\n",
    "        runningAccuracy = 0.\n",
    "        model.eval()\n",
    "        if savePreAndRealFlag:\n",
    "            preResult = []\n",
    "            realResult = []\n",
    "\n",
    "        topList = []\n",
    "        for inputs, labels in testLoader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            with torch.autograd.set_grad_enabled(False):\n",
    "                outputs = model(inputs)  # bsize * 2\n",
    "                loss = lossFn(outputs, labels)\n",
    "\n",
    "            if epoch>295:\n",
    "                softMax = nn.Softmax(dim=1)\n",
    "                out = softMax(outputs)\n",
    "                top = out.topk(1, dim=1)\n",
    "                topScore = top[0].cpu().numpy()\n",
    "                topList.append(topScore)\n",
    "                topFile = open(filePath+'fileOutput/' + '7wb_topScore.txt', 'w')\n",
    "                topsList = str(topList).replace(' ', '').replace(',dtype=float32)', '').replace(\"\\n\", '').replace(\n",
    "                    'array(', '').replace('[', '').replace('],', ' ').replace('],', ' ').replace(']', '').split(' ')\n",
    "                for sortNum in np.arange(len(topsList)):\n",
    "                    # tops = topsList[sortNum]\n",
    "                    tops = \"%06d\" % sortNum + ': ' + str(topsList[sortNum]) + '\\n'\n",
    "                    topFile.write(tops)\n",
    "                topFile.close()\n",
    "\n",
    "\n",
    "\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            runningLoss += loss.item() * inputs.size(0)\n",
    "            runningAccuracy += torch.sum(preds.view(-1) == labels.view(-1)).item()\n",
    "\n",
    "            if epoch>295:\n",
    "                if savePreAndRealFlag:\n",
    "                    preResult.append(str(preds.view(-1).cpu().numpy()))\n",
    "                    realResult.append(str(labels.view(-1).cpu().numpy()))\n",
    "        epochLoss = runningLoss / len(testLoader.dataset)\n",
    "        epochAcc = runningAccuracy / len(testLoader.dataset)\n",
    "        print(\"Epoch: {} Phase: {} loss: {}, acc: {}\".format(epoch, 'test', epochLoss, epochAcc))\n",
    "\n",
    "        if epochAcc > bestAccuracy:\n",
    "            bestAccuracy = epochAcc\n",
    "            bestModuleWeight = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        testLossHistory.append(epochLoss)\n",
    "        testAccuracyHistory.append(epochAcc)\n",
    "        if epoch % 5 == 0:\n",
    "            savePath2 = savePath + str(epoch) + '.pt'\n",
    "            torch.save(model.load_state_dict(bestModuleWeight), savePath2)\n",
    "    if epoch >295:\n",
    "        if savePreAndRealFlag:\n",
    "            preFile = open(filePath +'fileOutput/' + modelName + time.strftime('%m%d%H%M')+ '_wb12_pre.txt', 'w')\n",
    "            preFile.write(str(preResult))\n",
    "            realFile = open(filePath+'fileOutput/' + modelName + time.strftime('%m%d%H%M')+ '_wb12_real.txt', 'w')\n",
    "            realFile.write(str(realResult))\n",
    "            preFile.close()\n",
    "            realFile.close()\n",
    "    model.load_state_dict(bestModuleWeight)\n",
    "    return model, trainLossHistory, trainAccuracyHistory, testLossHistory, testAccuracyHistory\n",
    "\n",
    "\n",
    "modelUse = initialModel(modelName, numClasses, featureExtract=False, usePretrained=False)\n",
    "modelUse = modelUse.to(device)\n",
    "optimizer = torch.optim.Adam(modelUse.parameters(), lr=learningRate)\n",
    "lossFn = nn.CrossEntropyLoss()\n",
    "\n",
    "modelReturn, trHis, trAcc, teHis, teAcc = trainModel(modelUse, trainDataloader, testDataloader, lossFn, optimizer,\n",
    "                                                     numEpoch)\n",
    "torch.save(modelReturn, savePath + time.strftime('%m%d%H%M') + 'wb12_res01_batch16.pt')\n",
    "torch.save(modelReturn.state_dict, savePath + time.strftime('%m%d%H%M') + '_Para_res01_batch16.pt')\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "equal-webmaster",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
