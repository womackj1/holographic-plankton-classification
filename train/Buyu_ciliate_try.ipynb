{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "small-wound",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\lnyman2012\\appdata\\local\\continuum\\anaconda3\\envs\\automated_class\\lib\\site-packages\\torchvision\\transforms\\functional.py:92: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ..\\torch\\csrc\\utils\\tensor_numpy.cpp:141.)\n",
      "  img = torch.from_numpy(np.array(pic, np.float32, copy=False))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Phase: train loss: 1.4599323468126375, acc: 0.5250178994327257\n",
      "Epoch: 0 Phase: test loss: 12.45676884565575, acc: 0.026434629364467454\n",
      "Epoch: 1 Phase: train loss: 0.9218048001593638, acc: 0.6987800848157736\n",
      "Epoch: 1 Phase: test loss: 12.43991148100247, acc: 0.06333296618570328\n",
      "Epoch: 2 Phase: train loss: 0.705083882519148, acc: 0.7696343008206201\n",
      "Epoch: 2 Phase: test loss: 11.159229968268777, acc: 0.0636083269082498\n",
      "Epoch: 3 Phase: train loss: 0.5805555900570422, acc: 0.8088615960786474\n",
      "Epoch: 3 Phase: test loss: 25.286205526621465, acc: 0.06338803833021257\n",
      "Epoch: 4 Phase: train loss: 0.49184360981690056, acc: 0.8377485267389987\n",
      "Epoch: 4 Phase: test loss: 18.432950445780843, acc: 0.0635532547637405\n",
      "Epoch: 5 Phase: train loss: 0.4470332320608244, acc: 0.8524260615740485\n",
      "Epoch: 5 Phase: test loss: 21.850791876320535, acc: 0.06415904835334288\n",
      "Epoch: 6 Phase: train loss: 0.4106037897584249, acc: 0.8639230049016908\n",
      "Epoch: 6 Phase: test loss: 5.490968816513295, acc: 0.06085471968278445\n",
      "Epoch: 7 Phase: train loss: 0.3822055748256219, acc: 0.8748554276587542\n",
      "Epoch: 7 Phase: test loss: 14.728555947913119, acc: 0.03932151117964534\n",
      "Epoch: 8 Phase: train loss: 0.35713977983635137, acc: 0.8825659525251969\n",
      "Epoch: 8 Phase: test loss: 6.5911836628872384, acc: 0.06041414252670999\n",
      "Epoch: 9 Phase: train loss: 0.343937484308369, acc: 0.886944429145784\n",
      "Epoch: 9 Phase: test loss: 8.376350357033816, acc: 0.06404890406432426\n",
      "Epoch: 10 Phase: train loss: 0.32495156902783556, acc: 0.8930853114501295\n",
      "Epoch: 10 Phase: test loss: 9.810714145473005, acc: 0.0620112347174799\n",
      "Epoch: 11 Phase: train loss: 0.4209046443633888, acc: 0.861609847441758\n",
      "Epoch: 11 Phase: test loss: 64.58386636770354, acc: 0.07605463156735323\n",
      "Epoch: 12 Phase: train loss: 0.37886329984562495, acc: 0.876521451781682\n",
      "Epoch: 12 Phase: test loss: 18.859227600122868, acc: 0.06333296618570328\n",
      "Epoch: 13 Phase: train loss: 0.36271437043616994, acc: 0.8809136971966735\n",
      "Epoch: 13 Phase: test loss: 23.70827804324464, acc: 0.054135918052648974\n",
      "Epoch: 14 Phase: train loss: 0.3403913323188029, acc: 0.8883763837638377\n",
      "Epoch: 14 Phase: test loss: 5.926800821305162, acc: 0.09053860557330103\n",
      "Epoch: 15 Phase: train loss: 0.3289872510482307, acc: 0.893222999394173\n",
      "Epoch: 15 Phase: test loss: 16.971281258252922, acc: 0.06316774975217535\n",
      "Epoch: 16 Phase: train loss: 0.32737757506684767, acc: 0.8925345596739549\n",
      "Epoch: 16 Phase: test loss: 80.8053803301577, acc: 0.06305760546315674\n",
      "Epoch: 17 Phase: train loss: 0.3152625715451244, acc: 0.8971057994162032\n",
      "Epoch: 17 Phase: test loss: 14.691039812697465, acc: 0.006333296618570327\n",
      "Epoch: 18 Phase: train loss: 0.3066821426166314, acc: 0.8992399625488792\n",
      "Epoch: 18 Phase: test loss: 299.6007028271519, acc: 0.04989536292543232\n",
      "Epoch: 19 Phase: train loss: 0.29652866071869033, acc: 0.9035633639918489\n",
      "Epoch: 19 Phase: test loss: 4.084420512644043, acc: 0.06316774975217535\n",
      "Epoch: 20 Phase: train loss: 0.28763389085554014, acc: 0.9047750178994327\n",
      "Epoch: 20 Phase: test loss: 83.44298520893341, acc: 0.06570106839960348\n",
      "Epoch: 21 Phase: train loss: 0.33404303865043866, acc: 0.8910888362614969\n",
      "Epoch: 21 Phase: test loss: 32.57746117949315, acc: 0.1766714395858575\n",
      "Epoch: 22 Phase: train loss: 0.31293589481158085, acc: 0.8968854987057333\n",
      "Epoch: 22 Phase: test loss: 16.208921964951898, acc: 0.023075228549399713\n",
      "Epoch: 23 Phase: train loss: 0.30946498106063947, acc: 0.8977529327532081\n",
      "Epoch: 23 Phase: test loss: 35.9089068456268, acc: 0.18239894261482542\n",
      "Epoch: 24 Phase: train loss: 0.29373653117827253, acc: 0.9050090874043069\n",
      "Epoch: 24 Phase: test loss: 53.48188641181876, acc: 0.0543562066306862\n",
      "Epoch: 25 Phase: train loss: 0.28538301300850183, acc: 0.9065236547887867\n",
      "Epoch: 25 Phase: test loss: 1566.9562503776042, acc: 0.04631567353232735\n",
      "Epoch: 26 Phase: train loss: 0.30275602403431334, acc: 0.9022415597290301\n",
      "Epoch: 26 Phase: test loss: 103.18255443561969, acc: 0.05099680581561846\n",
      "Epoch: 27 Phase: train loss: 0.2883022493796313, acc: 0.9058902902461861\n",
      "Epoch: 27 Phase: test loss: 14.725372606574314, acc: 0.06344311047472188\n",
      "Epoch: 28 Phase: train loss: 0.27848670019950217, acc: 0.9093324888472766\n",
      "Epoch: 28 Phase: test loss: 5.265842551924555, acc: 0.0620112347174799\n",
      "Epoch: 29 Phase: train loss: 0.27481260571640914, acc: 0.9113978080079308\n",
      "Epoch: 29 Phase: test loss: 6.286919496647389, acc: 0.06283731688511951\n",
      "Epoch: 30 Phase: train loss: 0.2660302771362576, acc: 0.9123753924106405\n",
      "Epoch: 30 Phase: test loss: 29.96169203779143, acc: 0.07902852737085582\n",
      "Epoch: 31 Phase: train loss: 0.30105679449927686, acc: 0.9005066916340805\n",
      "Epoch: 31 Phase: test loss: 3.134316075175118, acc: 0.034420090318317\n",
      "Epoch: 32 Phase: train loss: 0.28973168775069136, acc: 0.9043344164784931\n",
      "Epoch: 32 Phase: test loss: 16.073973025922694, acc: 0.24699856812424276\n",
      "Epoch: 33 Phase: train loss: 0.2777617956455921, acc: 0.9089882689871674\n",
      "Epoch: 33 Phase: test loss: 12.136881567384325, acc: 0.04939971362484855\n",
      "Epoch: 34 Phase: train loss: 0.2751032508322503, acc: 0.9103513796331993\n",
      "Epoch: 34 Phase: test loss: 11.643494939420675, acc: 0.06377354334177773\n",
      "Epoch: 35 Phase: train loss: 0.2683917444480203, acc: 0.9129812193644324\n",
      "Epoch: 35 Phase: test loss: 13.108170308393454, acc: 0.03965194404670118\n",
      "Epoch: 36 Phase: train loss: 0.28238666236331667, acc: 0.9074186264250702\n",
      "Epoch: 36 Phase: test loss: 166.23654317199325, acc: 0.20712633549950435\n",
      "Epoch: 37 Phase: train loss: 0.27256602044647377, acc: 0.910681830698904\n",
      "Epoch: 37 Phase: test loss: 40.116050183136224, acc: 0.06327789404119397\n",
      "Epoch: 38 Phase: train loss: 0.2674695343975717, acc: 0.9130087569532411\n",
      "Epoch: 38 Phase: test loss: 24.768714156467517, acc: 0.1676946800308404\n",
      "Epoch: 39 Phase: train loss: 0.2610573189655918, acc: 0.9141102605055901\n",
      "Epoch: 39 Phase: test loss: 602.7183570429045, acc: 0.06316774975217535\n",
      "Epoch: 40 Phase: train loss: 0.251876073344141, acc: 0.9176350718731068\n",
      "Epoch: 40 Phase: test loss: 402.11697031393595, acc: 0.06316774975217535\n",
      "Epoch: 41 Phase: train loss: 0.28878975737532797, acc: 0.9058214462741643\n",
      "Epoch: 41 Phase: test loss: 84.86994836049165, acc: 0.06702279986782686\n",
      "Epoch: 42 Phase: train loss: 0.2692865866263112, acc: 0.9128710690091976\n",
      "Epoch: 42 Phase: test loss: 6.510962073505417, acc: 0.06476484194294525\n",
      "Epoch: 43 Phase: train loss: 0.26483150946842504, acc: 0.9120587101393401\n",
      "Epoch: 43 Phase: test loss: 28.76897946322118, acc: 0.056338803833021256\n",
      "Epoch: 44 Phase: train loss: 0.26287554451489875, acc: 0.9147023186649776\n",
      "Epoch: 44 Phase: test loss: 197.93550685980676, acc: 0.06316774975217535\n",
      "Epoch: 45 Phase: train loss: 0.25721508895483564, acc: 0.9171669328633585\n",
      "Epoch: 45 Phase: test loss: 148.598323921391, acc: 0.06316774975217535\n",
      "Epoch: 46 Phase: train loss: 0.28165688423515056, acc: 0.9074461640138789\n",
      "Epoch: 46 Phase: test loss: 160.17308853780506, acc: 0.05925762749201454\n",
      "Epoch: 47 Phase: train loss: 0.2716242781961162, acc: 0.9108745938205651\n",
      "Epoch: 47 Phase: test loss: 23.875696796850427, acc: 0.008976759555017073\n",
      "Epoch: 48 Phase: train loss: 0.26690156541417176, acc: 0.913421820785372\n",
      "Epoch: 48 Phase: test loss: 39.757635026451304, acc: 0.06316774975217535\n",
      "Epoch: 49 Phase: train loss: 0.2629438843743263, acc: 0.9137798094398855\n",
      "Epoch: 49 Phase: test loss: 9.502292426518242, acc: 0.20321621323934355\n",
      "Epoch: 50 Phase: train loss: 0.2562010754095751, acc: 0.9159415101613703\n",
      "Epoch: 50 Phase: test loss: 922.7272975312703, acc: 0.06316774975217535\n",
      "Epoch: 51 Phase: train loss: 0.28765601353537124, acc: 0.9069367186209175\n",
      "Epoch: 51 Phase: test loss: 16.043558395438055, acc: 0.040092521202775636\n",
      "Epoch: 52 Phase: train loss: 0.27379039391163734, acc: 0.9102274604835601\n",
      "Epoch: 52 Phase: test loss: 22.062703934098174, acc: 0.0520982487058046\n",
      "Epoch: 53 Phase: train loss: 0.2648699695275515, acc: 0.9125406179434928\n",
      "Epoch: 53 Phase: test loss: 12.388508846166573, acc: 0.06085471968278445\n",
      "Epoch: 54 Phase: train loss: 0.2596956791163653, acc: 0.9150740761138955\n",
      "Epoch: 54 Phase: test loss: 43.00787783598687, acc: 0.056889525278114327\n",
      "Epoch: 55 Phase: train loss: 0.2559472542087261, acc: 0.9178002974059591\n",
      "Epoch: 55 Phase: test loss: 18.49266506016077, acc: 0.06316774975217535\n",
      "Epoch: 56 Phase: train loss: 0.28159508700929053, acc: 0.9077766150795836\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 56 Phase: test loss: 200.07919944925368, acc: 0.06316774975217535\n",
      "Epoch: 57 Phase: train loss: 0.27188075419800356, acc: 0.9120036349617228\n",
      "Epoch: 57 Phase: test loss: 724.990027200573, acc: 0.01569556118515255\n",
      "Epoch: 58 Phase: train loss: 0.26450434214814933, acc: 0.9144820179545079\n",
      "Epoch: 58 Phase: test loss: 6.264366034829075, acc: 0.06322282189668466\n",
      "Epoch: 59 Phase: train loss: 0.2587868723744022, acc: 0.9155009087404307\n",
      "Epoch: 59 Phase: test loss: 5406.2600665816735, acc: 0.0627822447406102\n",
      "Epoch: 60 Phase: train loss: 0.2542264452432361, acc: 0.9162306548438619\n",
      "Epoch: 60 Phase: test loss: 51.506727332073055, acc: 0.06316774975217535\n",
      "Epoch: 61 Phase: train loss: 0.2858689352529404, acc: 0.9069091810321088\n",
      "Epoch: 61 Phase: test loss: 232.95385498054648, acc: 0.06041414252670999\n",
      "Epoch: 62 Phase: train loss: 0.27419325514717724, acc: 0.9104752987828386\n",
      "Epoch: 62 Phase: test loss: 52.14944605370925, acc: 0.06922568564819914\n",
      "Epoch: 63 Phase: train loss: 0.2656343301950313, acc: 0.9139588037671421\n",
      "Epoch: 63 Phase: test loss: 37178.49605739908, acc: 0.3698645225245071\n",
      "Epoch: 64 Phase: train loss: 0.2602215456394344, acc: 0.9151566888803216\n",
      "Epoch: 64 Phase: test loss: 1774.4843115311571, acc: 0.17221059588060358\n",
      "Epoch: 65 Phase: train loss: 0.2570735978248519, acc: 0.9168777881808668\n",
      "Epoch: 65 Phase: test loss: 46086.45933361455, acc: 0.006718801630135478\n",
      "Epoch: 66 Phase: train loss: 0.2662007902103774, acc: 0.914179104477612\n",
      "Epoch: 66 Phase: test loss: 159.25744259327251, acc: 0.01690714836435731\n",
      "Epoch: 67 Phase: train loss: 0.2545820630907507, acc: 0.9158588973949441\n",
      "Epoch: 67 Phase: test loss: 9329.197616372527, acc: 0.07627492014539046\n",
      "Epoch: 68 Phase: train loss: 0.2488436666455518, acc: 0.9185025059205816\n",
      "Epoch: 68 Phase: test loss: 189381.85896628862, acc: 0.06691265557880824\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms, models\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.autograd import Variable\n",
    "import torch.utils.data as Data\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from data import microPlankton\n",
    "import cv2 as cv\n",
    "from torchvision import transforms\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, train_test_split\n",
    "from dataTransformNew import dataTran\n",
    "import time\n",
    "\n",
    "batchSize = 32  # 11111111111\n",
    "numClasses = 17\n",
    "numEpoch = 300\n",
    "testSize = 0.2  # 11111111111\n",
    "learningRate = 0.01  # 11111111111\n",
    "dropNum = 0.5\n",
    "momentum = 0.9\n",
    "\n",
    "featureExtract = True\n",
    "usePretrained = False\n",
    "dropout = True\n",
    "mono = True\n",
    "savePreAndRealFlag = True\n",
    "\n",
    "samplePath = 'C:/Users/lnyman2012/Documents/Plankton_Image_Database_Local/Recon_only_for_Buyu_Zips/Zip_me_up/'\n",
    "inputSize = 400\n",
    "imgType = '.tif'\n",
    "txtFile = \"ciliate_training_data_v2.txt\"  # 11111111111\n",
    "modelName = \"shufflenet_v2_x1_5\"\n",
    "filePath = 'C:/automated_classification/holographic_plankton_classification-main/'\n",
    "savePath = 'C:/automated_classification/holographic_plankton_classification-main/modelGen/' + modelName + '_'\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "# --------------------find the input size for padding START\n",
    "def findMax(imgPath, fileType):\n",
    "    maxPixel = 0\n",
    "    for mainPath, dirs, file in os.walk(imgPath, topdown=False):\n",
    "        for subFolderName in dirs:  # read subfolder\n",
    "            pathNow = os.path.join(mainPath, subFolderName)\n",
    "            for subMainPath, subDirs, subFile in os.walk(pathNow, topdown=False):  # find data file\n",
    "                for dataFile in subFile:  # read data file\n",
    "                    if os.path.splitext(dataFile)[1] == fileType:  # find data with specific suffix\n",
    "                        img = cv.imread(subMainPath + '/' + dataFile)\n",
    "                        try:\n",
    "                            tempPixel = max(img.shape)\n",
    "                        except:\n",
    "                            print(dataFile)\n",
    "                        maxPixel = max(tempPixel, maxPixel)\n",
    "    return maxPixel\n",
    "\n",
    "\n",
    "# inputSize = findMax(samplePath, imgType)\n",
    "# --------------------find the input size for padding END\n",
    "\n",
    "dataTransforms = {\n",
    "    \"trainData\": transforms.Compose([\n",
    "        # transforms.Resize(inputSize),\n",
    "        # transforms.RandomCrop(size=inputSize, pad_if_needed=True, padding_mode='constant'),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),  # horizontal flip and 0.5 is the position\n",
    "        transforms.RandomVerticalFlip(p=0.5),\n",
    "        torchvision.transforms.RandomRotation(45, resample=False, expand=False,\n",
    "                                              center=None),\n",
    "        transforms.ToTensor(),\n",
    "        # transforms.Normalize([],[])\n",
    "    ]),\n",
    "    \"testData\": transforms.Compose([\n",
    "        # transforms.Resize(inputSize),\n",
    "        # transforms.RandomCrop(size=inputSize, pad_if_needed=True, padding_mode='constant'),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),  # horizontal flip and 0.5 is the position\n",
    "        transforms.RandomVerticalFlip(p=0.5),\n",
    "        transforms.ToTensor(),\n",
    "        # transforms.Normalize([],[])\n",
    "    ])\n",
    "}\n",
    "\n",
    "imagDataset = microPlankton(root=filePath, dataTXT=txtFile, )\n",
    "train_set, test_set = train_test_split(imagDataset, test_size=testSize, random_state=42)\n",
    "trainSet = dataTran(inputSize,train_set, transform=dataTransforms[\"trainData\"])\n",
    "testSet = dataTran(inputSize,test_set, transform=dataTransforms[\"testData\"])\n",
    "testDataloader = torch.utils.data.DataLoader(testSet, batch_size=batchSize, shuffle=True, num_workers=0,\n",
    "                                             pin_memory=True)\n",
    "trainDataloader = torch.utils.data.DataLoader(trainSet, batch_size=batchSize, shuffle=True, num_workers=0,\n",
    "                                              pin_memory=True)\n",
    "\n",
    "\n",
    "# -------------copy from resnet\n",
    "\n",
    "\n",
    "# --------------------Show imgs in the dataset Start\n",
    "# imgs = next(iter(trainDataloader['trainData']))[0]\n",
    "# unloader = transforms.ToPILImage()\n",
    "#\n",
    "# def imgShow( tensor, title):\n",
    "#     img = tensor.cpu().clone()\n",
    "#     img = img.squeeze(0)\n",
    "#     img = unloader(img)\n",
    "#     plt.imshow(img, cmap=\"gray\")\n",
    "#     if title is not None:\n",
    "#         plt.title(title)\n",
    "#     plt.pause(0.001)\n",
    "#\n",
    "# # print(imgs.shape)\n",
    "# plt.figure()\n",
    "# imgShow(imgs[3],title=\"img\")\n",
    "\n",
    "# --------------------Show imgs in the dataset End\n",
    "\n",
    "\n",
    "def setParameterRequiresGrad(model, featureExtract):\n",
    "    if featureExtract:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "\n",
    "def initialModel(modelName, numClasses, featureExtract, usePretrained):\n",
    "    if modelName == \"vgg19\":\n",
    "        modelUse = models.vgg19(pretrained=usePretrained)\n",
    "        setParameterRequiresGrad(modelUse, featureExtract)\n",
    "        if mono:\n",
    "            modelUse.features._modules['0'] = nn.Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "        modelUse.classifier._modules['6'] = nn.Linear(in_features=4096, out_features=numClasses, bias=True)\n",
    "\n",
    "    elif modelName == \"shufflenet_v2_x2_0\":\n",
    "        modelUse = models.shufflenet_v2_x2_0(pretrained=usePretrained)\n",
    "        setParameterRequiresGrad(modelUse, featureExtract)\n",
    "        if mono:\n",
    "            modelUse.conv1._modules['0'] = nn.Conv2d(1, 24, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
    "\n",
    "        if dropout:\n",
    "            modelUse.fc = nn.Sequential(nn.Dropout(dropNum),\n",
    "                                        nn.Linear(2048, numClasses, bias=True))\n",
    "        else:\n",
    "            modelUse.fc = nn.Linear(2048, numClasses, bias=True)\n",
    "\n",
    "    elif modelName == \"shufflenet_v2_x0_5\":\n",
    "        modelUse = models.shufflenet_v2_x0_5(pretrained=usePretrained)\n",
    "        setParameterRequiresGrad(modelUse, featureExtract)\n",
    "        if mono:\n",
    "            modelUse.conv1._modules['0'] = nn.Conv2d(1, 24, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
    "\n",
    "        if dropout:\n",
    "            modelUse.fc = nn.Sequential(nn.Dropout(dropNum),\n",
    "                                        nn.Linear(1024, numClasses, bias=True),\n",
    "                                        )\n",
    "        else:\n",
    "            modelUse.fc = nn.Linear(1024, numClasses, bias=True)\n",
    "\n",
    "\n",
    "    elif modelName == \"shufflenet_v2_x1_5\":\n",
    "        modelUse = models.shufflenet_v2_x1_5(pretrained=usePretrained)\n",
    "        setParameterRequiresGrad(modelUse, featureExtract)\n",
    "        if mono:\n",
    "            modelUse.conv1._modules['0'] = nn.Conv2d(1, 24, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
    "\n",
    "        if dropout:\n",
    "            modelUse.fc = nn.Sequential(nn.Dropout(dropNum),\n",
    "                                        nn.Linear(1024, numClasses, bias=True),)\n",
    "        else:\n",
    "            modelUse.fc = nn.Linear(1024, numClasses, bias=True)\n",
    "\n",
    "\n",
    "    elif modelName == \"shufflenet_v2_x1_0\":\n",
    "        modelUse = models.shufflenet_v2_x1_0(pretrained=usePretrained)\n",
    "        setParameterRequiresGrad(modelUse, featureExtract)\n",
    "        if mono:\n",
    "            modelUse.conv1._modules['0'] = nn.Conv2d(1, 24, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
    "\n",
    "        if dropout:\n",
    "            modelUse.fc = nn.Sequential(nn.Dropout(dropNum),\n",
    "                                        nn.Linear(1024, numClasses, bias=True))\n",
    "        else:\n",
    "            modelUse.fc = nn.Linear(1024, numClasses, bias=True)\n",
    "\n",
    "\n",
    "    elif modelName == \"squeezenet1_1\":\n",
    "        modelUse = models.squeezenet1_1(pretrained=usePretrained)\n",
    "        setParameterRequiresGrad(modelUse, featureExtract)\n",
    "        if mono:\n",
    "            modelUse.features._modules['0'] = nn.Conv2d(1, 64, kernel_size=(3, 3), stride=(2, 2))\n",
    "\n",
    "        modelUse.classifier = nn.Sequential(nn.Dropout(p=dropNum, inplace=False),\n",
    "                                nn.Conv2d(512, numClasses, kernel_size=(1, 1), stride=(1, 1)),\n",
    "                                nn.ReLU(inplace=True),\n",
    "                                nn.AdaptiveAvgPool2d(output_size=(1, 1)))\n",
    "\n",
    "\n",
    "\n",
    "    elif modelName == \"densenet121\":\n",
    "        modelUse = models.densenet121(pretrained=usePretrained)\n",
    "        setParameterRequiresGrad(modelUse, featureExtract)\n",
    "        if mono:\n",
    "            modelUse.features._modules['conv0'] = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "\n",
    "        if dropout:\n",
    "            modelUse.classifier = nn.Sequential(nn.Dropout(dropNum),\n",
    "                                        nn.Linear(1024, numClasses, bias=True))\n",
    "        else:\n",
    "            modelUse.classifier = nn.Linear(1024, numClasses, bias=True)\n",
    "\n",
    "\n",
    "    elif modelName == \"vgg16\":\n",
    "        modelUse = models.vgg16(pretrained=usePretrained)\n",
    "        setParameterRequiresGrad(modelUse, featureExtract)\n",
    "        if mono:\n",
    "            modelUse.features._modules['0'] = nn.Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "        modelUse.classifier._modules['6'] = nn.Linear(in_features=4096, out_features=numClasses, bias=True)\n",
    "\n",
    "    elif modelName == 'mobilenet_v2':\n",
    "        modelUse = models.mobilenet_v2(pretrained=usePretrained)\n",
    "        setParameterRequiresGrad(modelUse, featureExtract)\n",
    "        if mono:\n",
    "            modelUse.features._modules['0']._modules['0'] = nn.Conv2d(1, 32, kernel_size=(3, 3), stride=(2, 2),\n",
    "                                                                      padding=(1, 1), bias=False)\n",
    "\n",
    "        modelUse.classifier = nn.Sequential(\n",
    "            nn.Dropout(p=dropNum, inplace=False),\n",
    "            nn.Linear(in_features=1280, out_features=numClasses, bias=True)\n",
    "        )\n",
    "\n",
    "    elif modelName == \"resnet18\":\n",
    "        modelUse = models.resnet18(pretrained=usePretrained)\n",
    "        setParameterRequiresGrad(modelUse, featureExtract)\n",
    "        numFeatures = modelUse.fc.in_features\n",
    "        if mono:\n",
    "            modelUse.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "\n",
    "        if dropout:\n",
    "            modelUse.fc = nn.Sequential(nn.Dropout(dropNum),\n",
    "                                        nn.Linear(numFeatures, numClasses))\n",
    "        else:\n",
    "            modelUse.fc = nn.Linear(numFeatures, numClasses)\n",
    "\n",
    "    elif modelName == \"resnet34\":\n",
    "        modelUse = models.resnet34(pretrained=usePretrained)\n",
    "        setParameterRequiresGrad(modelUse, featureExtract)\n",
    "        numFeatures = modelUse.fc.in_features\n",
    "        if mono:\n",
    "            modelUse.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "\n",
    "        if dropout:\n",
    "            modelUse.fc = nn.Sequential(nn.Dropout(dropNum),\n",
    "                                        nn.Linear(numFeatures, numClasses))\n",
    "        else:\n",
    "            modelUse.fc = nn.Linear(numFeatures, numClasses)\n",
    "\n",
    "    elif modelName == \"resnet50\":\n",
    "        modelUse = models.resnet50(pretrained=usePretrained)\n",
    "        setParameterRequiresGrad(modelUse, featureExtract)\n",
    "        numFeatures = modelUse.fc.in_features\n",
    "        if mono:\n",
    "            modelUse.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "\n",
    "        if dropout:\n",
    "            modelUse.fc = nn.Sequential(nn.Dropout(dropNum),\n",
    "                                        nn.Linear(numFeatures, numClasses))\n",
    "        else:\n",
    "            modelUse.fc = nn.Linear(numFeatures, numClasses)\n",
    "\n",
    "    else:\n",
    "        print(\"model not implemented\")\n",
    "        return None, None\n",
    "\n",
    "    return modelUse\n",
    "\n",
    "\n",
    "def trainModel(model, trainLoader, testLoader, lossFn, optimizer, numEpochs):\n",
    "    # if len(trainLoader) > len(testLoader):\n",
    "    #     phase = 'trai'\n",
    "    bestAccuracy = 0\n",
    "    bestModuleWeight = copy.deepcopy(model.state_dict())\n",
    "    trainAccuracyHistory = []\n",
    "    trainLossHistory = []\n",
    "    testAccuracyHistory = []\n",
    "    testLossHistory = []\n",
    "    for epoch in np.arange(numEpochs):\n",
    "        runningLoss = 0.\n",
    "        runningAccuracy = 0.\n",
    "        model.train()\n",
    "        for inputs, labels in trainLoader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            with torch.autograd.set_grad_enabled(True):\n",
    "                outputs = model(inputs)  # bsize * 2\n",
    "                loss = lossFn(outputs, labels)\n",
    "\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            runningLoss += loss.item() * inputs.size(0)\n",
    "            runningAccuracy += torch.sum(preds.view(-1) == labels.view(-1)).item()\n",
    "        epochLoss = runningLoss / len(trainLoader.dataset)\n",
    "        epochAcc = runningAccuracy / len(trainLoader.dataset)\n",
    "        print(\"Epoch: {} Phase: {} loss: {}, acc: {}\".format(epoch, 'train', epochLoss, epochAcc))\n",
    "\n",
    "        trainLossHistory.append(epochLoss)\n",
    "        trainAccuracyHistory.append(epochAcc)\n",
    "\n",
    "        # bestAccuracy = 0\n",
    "        # bestModuleWeight = copy.deepcopy(model.state_dict())\n",
    "        # trainAccuracyHistory = []\n",
    "        # trainLossHistory = []\n",
    "        # testAccuracyHistory = []\n",
    "        # testLossHistory = []\n",
    "\n",
    "        runningLoss = 0.\n",
    "        runningAccuracy = 0.\n",
    "        model.eval()\n",
    "        if savePreAndRealFlag:\n",
    "            preResult = []\n",
    "            realResult = []\n",
    "\n",
    "        topList = []\n",
    "        for inputs, labels in testLoader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            with torch.autograd.set_grad_enabled(False):\n",
    "                outputs = model(inputs)  # bsize * 2\n",
    "                loss = lossFn(outputs, labels)\n",
    "\n",
    "            if epoch>295:\n",
    "                softMax = nn.Softmax(dim=1)\n",
    "                out = softMax(outputs)\n",
    "                top = out.topk(1, dim=1)\n",
    "                topScore = top[0].cpu().numpy()\n",
    "                topList.append(topScore)\n",
    "                topFile = open(filePath+'fileOutput/' + '10wbmerge_topScore.txt', 'w')\n",
    "                topsList = str(topList).replace(' ', '').replace(',dtype=float32)', '').replace(\"\\n\", '').replace(\n",
    "                    'array(', '').replace('[', '').replace('],', ' ').replace('],', ' ').replace(']', '').split(' ')\n",
    "                for sortNum in np.arange(len(topsList)):\n",
    "                    # tops = topsList[sortNum]\n",
    "                    tops = \"%06d\" % sortNum + ': ' + str(topsList[sortNum]) + '\\n'\n",
    "                    topFile.write(tops)\n",
    "                topFile.close()\n",
    "\n",
    "\n",
    "\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            runningLoss += loss.item() * inputs.size(0)\n",
    "            runningAccuracy += torch.sum(preds.view(-1) == labels.view(-1)).item()\n",
    "\n",
    "            if epoch>295:\n",
    "                if savePreAndRealFlag:\n",
    "                    preResult.append(str(preds.view(-1).cpu().numpy()))\n",
    "                    realResult.append(str(labels.view(-1).cpu().numpy()))\n",
    "        epochLoss = runningLoss / len(testLoader.dataset)\n",
    "        epochAcc = runningAccuracy / len(testLoader.dataset)\n",
    "        print(\"Epoch: {} Phase: {} loss: {}, acc: {}\".format(epoch, 'test', epochLoss, epochAcc))\n",
    "\n",
    "        if epochAcc > bestAccuracy:\n",
    "            bestAccuracy = epochAcc\n",
    "            bestModuleWeight = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        testLossHistory.append(epochLoss)\n",
    "        testAccuracyHistory.append(epochAcc)\n",
    "        if epoch % 5 == 0:\n",
    "            savePath2 = savePath + str(epoch) + '.pt'\n",
    "            torch.save(model.load_state_dict(bestModuleWeight), savePath2)\n",
    "    if epoch >295:\n",
    "        if savePreAndRealFlag:\n",
    "            preFile = open(filePath +'fileOutput/' + modelName + time.strftime('%m%d%H%M')+ '_wb12_pre.txt', 'w')\n",
    "            preFile.write(str(preResult))\n",
    "            realFile = open(filePath+'fileOutput/' + modelName + time.strftime('%m%d%H%M')+ '_wb12_real.txt', 'w')\n",
    "            realFile.write(str(realResult))\n",
    "            preFile.close()\n",
    "            realFile.close()\n",
    "    model.load_state_dict(bestModuleWeight)\n",
    "    return model, trainLossHistory, trainAccuracyHistory, testLossHistory, testAccuracyHistory\n",
    "\n",
    "\n",
    "modelUse = initialModel(modelName, numClasses, featureExtract=False, usePretrained=False)\n",
    "modelUse = modelUse.to(device)\n",
    "optimizer = torch.optim.Adam(modelUse.parameters(), lr=learningRate)\n",
    "lossFn = nn.CrossEntropyLoss()\n",
    "\n",
    "modelReturn, trHis, trAcc, teHis, teAcc = trainModel(modelUse, trainDataloader, testDataloader, lossFn, optimizer,\n",
    "                                                     numEpoch)\n",
    "torch.save(modelReturn, savePath + time.strftime('%m%d%H%M') + 'wb12_res01_batch16.pt')\n",
    "torch.save(modelReturn.state_dict, savePath + time.strftime('%m%d%H%M') + '_Para_res01_batch16.pt')\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "strategic-college",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visible-aaron",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
